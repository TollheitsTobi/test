{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def get_env_data(n):\n",
    "    env = gym.make(\"ALE/Freeway-v5\", render_mode=\"rgb_array\", obs_type=\"ram\", difficulty=1, mode=3)\n",
    "    observation = env.reset()\n",
    "\n",
    "    df = pd.DataFrame([observation])\n",
    "    # Actions: 0: nichts, 1: up, 2: down\n",
    "\n",
    "    actions = []\n",
    "\n",
    "    for i in range(n):\n",
    "        action = get_action_sample()\n",
    "        if i != 0:\n",
    "            df.loc[len(df)] = observation\n",
    "        actions.append(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "        if i % (n / 100) == 0:\n",
    "            print(f\"{(i / n) * 100}%\")\n",
    "    env.close()\n",
    "    df = df[[14, 103, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]]\n",
    "    df[\"actions\"] = actions\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def get_action_sample():\n",
    "    x = random.randint(0, 101)\n",
    "    if x < 90:\n",
    "        return 1\n",
    "    if x < 97:\n",
    "        return 2\n",
    "    return 0\n",
    "GAME_START = [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(14, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        #x = torch.FloatTensor(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def get_dfs(df):\n",
    "    df = df.tail(len(df) - 1)\n",
    "\n",
    "    dfY = df.copy()\n",
    "    dfY.drop([\"actions\"], axis=1, inplace=True)\n",
    "\n",
    "    dfY = dfY.drop(dfY.index[[0]])\n",
    "    df = df.drop(df.index[[len(df) - 1]])\n",
    "    dfY.index = df.index\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    dfY = dfY.reset_index(drop=True)\n",
    "    return df, dfY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def train(X, y, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    batch_size = 32\n",
    "    for i in range(len(y)):\n",
    "        X_data = list(X.iloc[i])\n",
    "        y_data = list(y.iloc[i])\n",
    "        X_data = torch.tensor(X_data).cuda()\n",
    "        y_data = torch.tensor(y_data).cuda()\n",
    "\n",
    "        pred = model.forward(X_data.float())\n",
    "\n",
    "        loss = loss_fn(pred.to(torch.float32), y_data.to(torch.float32))\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"loss: {loss_sum / 1000}\")\n",
    "            loss_sum = 0\n",
    "\n",
    "\n",
    "def test(X, y, model, loss_fn):\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(y)):\n",
    "            X_data = list(X.iloc[i])\n",
    "            y_data = list(y.iloc[i])\n",
    "            X_data = torch.tensor(X_data).cuda()\n",
    "            y_data = torch.tensor(y_data).cuda()\n",
    "\n",
    "            pred = model.forward(X_data.float())\n",
    "            loss = loss_fn(pred, y_data)\n",
    "            loss_sum += loss\n",
    "\n",
    "    loss_sum /= len(y)\n",
    "\n",
    "    print(f\"Avg loss: {loss_sum}!\")\n",
    "    return loss_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def cv_model(X, y):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "    model = Net().cuda()\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    last_test_avg = 10000\n",
    "    test_avg = 0\n",
    "    overfit = 0\n",
    "\n",
    "    epochs = 20000\n",
    "\n",
    "    avg_losses = []\n",
    "\n",
    "    avg_losses.append(test(test_X, test_y, model, loss_fn))\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t + 1}-----------------------------\")\n",
    "        train(train_X, train_y, model, loss_fn, optimizer)\n",
    "        test_avg = test(test_X, test_y, model, loss_fn)\n",
    "        avg_losses.append(test_avg)\n",
    "        if test_avg > last_test_avg:\n",
    "            overfit += 1\n",
    "        else:\n",
    "            overfit = 0\n",
    "            last_test_avg = test_avg\n",
    "        if overfit >= 20:\n",
    "            print(f\"Epoche: {t}\")\n",
    "            break\n",
    "    torch.save(model, f\"game_model/game_model_1\")\n",
    "    return avg_losses\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "Avg loss: 6728.08837890625!\n",
      "Epoch 1-----------------------------\n",
      "loss: 6.65534228515625\n",
      "loss: 6667.668668241501\n",
      "loss: 6618.763896484375\n",
      "loss: 6539.144665039063\n",
      "Avg loss: 6430.09375!\n",
      "Epoch 2-----------------------------\n",
      "loss: 6.404578125\n",
      "loss: 6315.120605993271\n",
      "loss: 6140.1093515625\n",
      "loss: 5918.372184082031\n",
      "Avg loss: 5678.767578125!\n",
      "Epoch 3-----------------------------\n",
      "loss: 5.74822265625\n",
      "loss: 5468.597339950562\n",
      "loss: 5098.823958251953\n",
      "loss: 4707.692186279297\n",
      "Avg loss: 4380.43017578125!\n",
      "Epoch 4-----------------------------\n",
      "loss: 4.5786376953125\n",
      "loss: 4159.171546508789\n",
      "loss: 3821.880580566406\n",
      "loss: 3496.618873535156\n",
      "Avg loss: 3311.83154296875!\n",
      "Epoch 5-----------------------------\n",
      "loss: 3.307545166015625\n",
      "loss: 3201.3200161132813\n",
      "loss: 3100.582257446289\n",
      "loss: 3040.185616821289\n",
      "Avg loss: 2997.701904296875!\n",
      "Epoch 6-----------------------------\n",
      "loss: 2.68127294921875\n",
      "loss: 2950.5815877685545\n",
      "loss: 2932.5497436523438\n",
      "loss: 2915.6361466064454\n",
      "Avg loss: 2896.9228515625!\n",
      "Epoch 7-----------------------------\n",
      "loss: 2.435023193359375\n",
      "loss: 2860.0005047607424\n",
      "loss: 2861.880856201172\n",
      "loss: 2856.552829589844\n",
      "Avg loss: 2845.150634765625!\n",
      "Epoch 8-----------------------------\n",
      "loss: 2.314467041015625\n",
      "loss: 2810.287923339844\n",
      "loss: 2819.3536357421876\n",
      "loss: 2817.1331119384768\n",
      "Avg loss: 2809.4560546875!\n",
      "Epoch 9-----------------------------\n",
      "loss: 2.25517138671875\n",
      "loss: 2774.650621459961\n",
      "loss: 2786.9220505371095\n",
      "loss: 2784.839768066406\n",
      "Avg loss: 2778.791259765625!\n",
      "Epoch 10-----------------------------\n",
      "loss: 2.221241455078125\n",
      "loss: 2742.472993713379\n",
      "loss: 2753.314973510742\n",
      "loss: 2744.9602841186525\n",
      "Avg loss: 2740.056884765625!\n",
      "Epoch 11-----------------------------\n",
      "loss: 2.195663330078125\n",
      "loss: 2700.7437825317384\n",
      "loss: 2710.6810087280273\n",
      "loss: 2704.1564561157224\n",
      "Avg loss: 2702.244873046875!\n",
      "Epoch 12-----------------------------\n",
      "loss: 2.180755859375\n",
      "loss: 2659.940734436035\n",
      "loss: 2668.2166044921873\n",
      "loss: 2659.5563500976564\n",
      "Avg loss: 2658.765869140625!\n",
      "Epoch 13-----------------------------\n",
      "loss: 2.172026611328125\n",
      "loss: 2612.2892291259764\n",
      "loss: 2618.4897424926758\n",
      "loss: 2606.724973815918\n",
      "Avg loss: 2608.6865234375!\n",
      "Epoch 14-----------------------------\n",
      "loss: 2.161035400390625\n",
      "loss: 2558.5866130371096\n",
      "loss: 2564.5542180786133\n",
      "loss: 2551.82631652832\n",
      "Avg loss: 2556.984130859375!\n",
      "Epoch 15-----------------------------\n",
      "loss: 2.158921875\n",
      "loss: 2502.811983947754\n",
      "loss: 2508.331291381836\n",
      "loss: 2495.037570861816\n",
      "Avg loss: 2504.001953125!\n",
      "Epoch 16-----------------------------\n",
      "loss: 2.1612314453125\n",
      "loss: 2445.710684753418\n",
      "loss: 2452.4927713623047\n",
      "loss: 2440.3990075073243\n",
      "Avg loss: 2454.3359375!\n",
      "Epoch 17-----------------------------\n",
      "loss: 2.166859130859375\n",
      "loss: 2392.194642944336\n",
      "loss: 2400.4428637695314\n",
      "loss: 2389.726375\n",
      "Avg loss: 2408.5341796875!\n",
      "Epoch 18-----------------------------\n",
      "loss: 2.177468505859375\n",
      "loss: 2342.484497131348\n",
      "loss: 2352.944209411621\n",
      "loss: 2344.159976074219\n",
      "Avg loss: 2367.760986328125!\n",
      "Epoch 19-----------------------------\n",
      "loss: 2.18956787109375\n",
      "loss: 2298.4539372558593\n",
      "loss: 2310.7397984619142\n",
      "loss: 2303.9107723999023\n",
      "Avg loss: 2331.077880859375!\n",
      "Epoch 20-----------------------------\n",
      "loss: 2.212978515625\n",
      "loss: 2258.9340498657225\n",
      "loss: 2273.1487173461915\n",
      "loss: 2268.5811091918945\n",
      "Avg loss: 2299.755615234375!\n",
      "Epoch 21-----------------------------\n",
      "loss: 2.239\n",
      "loss: 2225.4925354614256\n",
      "loss: 2241.6821812744142\n",
      "loss: 2239.412327026367\n",
      "Avg loss: 2273.524658203125!\n",
      "Epoch 22-----------------------------\n",
      "loss: 2.267036376953125\n",
      "loss: 2197.639043701172\n",
      "loss: 2215.2173137207033\n",
      "loss: 2214.9704494018556\n",
      "Avg loss: 2251.310302734375!\n",
      "Epoch 23-----------------------------\n",
      "loss: 2.293962890625\n",
      "loss: 2174.1370560913088\n",
      "loss: 2192.670867614746\n",
      "loss: 2193.6536302490235\n",
      "Avg loss: 2231.157470703125!\n",
      "Epoch 24-----------------------------\n",
      "loss: 2.32407470703125\n",
      "loss: 2153.095538330078\n",
      "loss: 2172.3661709594726\n",
      "loss: 2174.3566604003904\n",
      "Avg loss: 2212.550048828125!\n",
      "Epoch 25-----------------------------\n",
      "loss: 2.359288330078125\n",
      "loss: 2133.977908752441\n",
      "loss: 2153.568035095215\n",
      "loss: 2156.284029663086\n",
      "Avg loss: 2194.86181640625!\n",
      "Epoch 26-----------------------------\n",
      "loss: 2.396619140625\n",
      "loss: 2116.079560058594\n",
      "loss: 2135.6792728271485\n",
      "loss: 2139.0326568603514\n",
      "Avg loss: 2177.785888671875!\n",
      "Epoch 27-----------------------------\n",
      "loss: 2.439099365234375\n",
      "loss: 2099.0505576782225\n",
      "loss: 2118.5398177490233\n",
      "loss: 2122.129110473633\n",
      "Avg loss: 2160.81298828125!\n",
      "Epoch 28-----------------------------\n",
      "loss: 2.48348095703125\n",
      "loss: 2082.307544555664\n",
      "loss: 2101.7960414428712\n",
      "loss: 2105.8044824829103\n",
      "Avg loss: 2144.391845703125!\n",
      "Epoch 29-----------------------------\n",
      "loss: 2.532639892578125\n",
      "loss: 2066.254130737305\n",
      "loss: 2085.598231750488\n",
      "loss: 2090.0769473876953\n",
      "Avg loss: 2128.441650390625!\n",
      "Epoch 30-----------------------------\n",
      "loss: 2.58647119140625\n",
      "loss: 2050.872264160156\n",
      "loss: 2070.099049255371\n",
      "loss: 2074.7281959838865\n",
      "Avg loss: 2112.86181640625!\n",
      "Epoch 31-----------------------------\n",
      "loss: 2.64792529296875\n",
      "loss: 2035.926299621582\n",
      "loss: 2054.995480895996\n",
      "loss: 2059.957297241211\n",
      "Avg loss: 2097.780029296875!\n",
      "Epoch 32-----------------------------\n",
      "loss: 2.712276611328125\n",
      "loss: 2021.5985013427735\n",
      "loss: 2040.4564351806641\n",
      "loss: 2045.947530883789\n",
      "Avg loss: 2083.662841796875!\n",
      "Epoch 33-----------------------------\n",
      "loss: 2.7759677734375\n",
      "loss: 2008.1928180541993\n",
      "loss: 2026.776157104492\n",
      "loss: 2032.6295021362305\n",
      "Avg loss: 2069.966552734375!\n",
      "Epoch 34-----------------------------\n",
      "loss: 2.841302490234375\n",
      "loss: 1995.3167525634765\n",
      "loss: 2013.7621897583008\n",
      "loss: 2020.12059564209\n",
      "Avg loss: 2057.2861328125!\n",
      "Epoch 35-----------------------------\n",
      "loss: 2.90950830078125\n",
      "loss: 1983.3921868896484\n",
      "loss: 2001.4716934814453\n",
      "loss: 2007.9779501953126\n",
      "Avg loss: 2044.86474609375!\n",
      "Epoch 36-----------------------------\n",
      "loss: 2.9787353515625\n",
      "loss: 1971.8079739379882\n",
      "loss: 1990.0243530273438\n",
      "loss: 1996.9429818725587\n",
      "Avg loss: 2033.6444091796875!\n",
      "Epoch 37-----------------------------\n",
      "loss: 3.049251220703125\n",
      "loss: 1961.2702302856446\n",
      "loss: 1979.1954286499024\n",
      "loss: 1986.4743760375977\n",
      "Avg loss: 2022.9432373046875!\n",
      "Epoch 38-----------------------------\n",
      "loss: 3.118385986328125\n",
      "loss: 1951.1649013061524\n",
      "loss: 1968.9555864868164\n",
      "loss: 1976.6443489379883\n",
      "Avg loss: 2013.0611572265625!\n",
      "Epoch 39-----------------------------\n",
      "loss: 3.18343408203125\n",
      "loss: 1941.85169732666\n",
      "loss: 1959.449233581543\n",
      "loss: 1967.4783068237305\n",
      "Avg loss: 2003.75634765625!\n",
      "Epoch 40-----------------------------\n",
      "loss: 3.2395615234375\n",
      "loss: 1933.054831237793\n",
      "loss: 1950.5682274780274\n",
      "loss: 1958.949226928711\n",
      "Avg loss: 1995.05517578125!\n",
      "Epoch 41-----------------------------\n",
      "loss: 3.28993408203125\n",
      "loss: 1924.8335022888184\n",
      "loss: 1942.2550746765137\n",
      "loss: 1950.7959446411132\n",
      "Avg loss: 1987.04248046875!\n",
      "Epoch 42-----------------------------\n",
      "loss: 3.33839501953125\n",
      "loss: 1917.1618469543457\n",
      "loss: 1934.4430579833984\n",
      "loss: 1943.1566644897462\n",
      "Avg loss: 1979.4249267578125!\n",
      "Epoch 43-----------------------------\n",
      "loss: 3.384197021484375\n",
      "loss: 1909.899549407959\n",
      "loss: 1927.1769009399413\n",
      "loss: 1936.1017035522461\n",
      "Avg loss: 1972.3465576171875!\n",
      "Epoch 44-----------------------------\n",
      "loss: 3.4260849609375\n",
      "loss: 1903.010520904541\n",
      "loss: 1920.146098022461\n",
      "loss: 1929.182851928711\n",
      "Avg loss: 1965.4671630859375!\n",
      "Epoch 45-----------------------------\n",
      "loss: 3.461552490234375\n",
      "loss: 1896.3274575805665\n",
      "loss: 1913.266573486328\n",
      "loss: 1922.3414167785645\n",
      "Avg loss: 1958.8017578125!\n",
      "Epoch 46-----------------------------\n",
      "loss: 3.492048095703125\n",
      "loss: 1889.898893157959\n",
      "loss: 1906.7247074890138\n",
      "loss: 1916.010182281494\n",
      "Avg loss: 1952.575927734375!\n",
      "Epoch 47-----------------------------\n",
      "loss: 3.518654052734375\n",
      "loss: 1883.78973828125\n",
      "loss: 1900.575900390625\n",
      "loss: 1909.8473636169433\n",
      "Avg loss: 1946.571533203125!\n",
      "Epoch 48-----------------------------\n",
      "loss: 3.541068603515625\n",
      "loss: 1877.8965537414551\n",
      "loss: 1894.4871249389648\n",
      "loss: 1903.642406524658\n",
      "Avg loss: 1940.5352783203125!\n",
      "Epoch 49-----------------------------\n",
      "loss: 3.55779248046875\n",
      "loss: 1871.933412475586\n",
      "loss: 1888.5943844909668\n",
      "loss: 1897.541375579834\n",
      "Avg loss: 1934.5782470703125!\n",
      "Epoch 50-----------------------------\n",
      "loss: 3.575537353515625\n",
      "loss: 1866.033455871582\n",
      "loss: 1882.5360928955079\n",
      "loss: 1891.4003588562011\n",
      "Avg loss: 1928.622802734375!\n",
      "Epoch 51-----------------------------\n",
      "loss: 3.5846533203125\n",
      "loss: 1860.1228659057617\n",
      "loss: 1876.535624938965\n",
      "loss: 1884.9466167602538\n",
      "Avg loss: 1922.172119140625!\n",
      "Epoch 52-----------------------------\n",
      "loss: 3.591267578125\n",
      "loss: 1853.551242034912\n",
      "loss: 1870.3658923339844\n",
      "loss: 1878.560845703125\n",
      "Avg loss: 1916.0526123046875!\n",
      "Epoch 53-----------------------------\n",
      "loss: 3.600765380859375\n",
      "loss: 1847.5097338256835\n",
      "loss: 1864.4453006896972\n",
      "loss: 1872.644201324463\n",
      "Avg loss: 1910.3309326171875!\n",
      "Epoch 54-----------------------------\n",
      "loss: 3.6045361328125\n",
      "loss: 1841.705537902832\n",
      "loss: 1858.5937362976074\n",
      "loss: 1866.616873626709\n",
      "Avg loss: 1904.41064453125!\n",
      "Epoch 55-----------------------------\n",
      "loss: 3.601239013671875\n",
      "loss: 1835.8042169189453\n",
      "loss: 1852.9299604797363\n",
      "loss: 1860.7693741149903\n",
      "Avg loss: 1898.6888427734375!\n",
      "Epoch 56-----------------------------\n",
      "loss: 3.597283447265625\n",
      "loss: 1830.0440241394042\n",
      "loss: 1847.335100341797\n",
      "loss: 1854.6429096679688\n",
      "Avg loss: 1892.8480224609375!\n",
      "Epoch 57-----------------------------\n",
      "loss: 3.577462158203125\n",
      "loss: 1824.1642705993652\n",
      "loss: 1841.5067344055176\n",
      "loss: 1848.492595123291\n",
      "Avg loss: 1886.8822021484375!\n",
      "Epoch 58-----------------------------\n",
      "loss: 3.56404345703125\n",
      "loss: 1818.2489364624023\n",
      "loss: 1835.7522446289063\n",
      "loss: 1842.365723449707\n",
      "Avg loss: 1881.0198974609375!\n",
      "Epoch 59-----------------------------\n",
      "loss: 3.550645751953125\n",
      "loss: 1812.4302016296388\n",
      "loss: 1830.2216885375976\n",
      "loss: 1836.5369716186524\n",
      "Avg loss: 1875.4544677734375!\n",
      "Epoch 60-----------------------------\n",
      "loss: 3.53946923828125\n",
      "loss: 1806.8577086486816\n",
      "loss: 1824.6697992553711\n",
      "loss: 1830.6599324951171\n",
      "Avg loss: 1869.9053955078125!\n",
      "Epoch 61-----------------------------\n",
      "loss: 3.523032958984375\n",
      "loss: 1801.2262043151854\n",
      "loss: 1819.2217786254882\n",
      "loss: 1824.918160736084\n",
      "Avg loss: 1864.482666015625!\n",
      "Epoch 62-----------------------------\n",
      "loss: 3.510134033203125\n",
      "loss: 1795.7051182250977\n",
      "loss: 1814.0676358947753\n",
      "loss: 1819.2652457885742\n",
      "Avg loss: 1859.3228759765625!\n",
      "Epoch 63-----------------------------\n",
      "loss: 3.497802978515625\n",
      "loss: 1790.4452434692382\n",
      "loss: 1808.9110029296876\n",
      "loss: 1813.803783416748\n",
      "Avg loss: 1854.2027587890625!\n",
      "Epoch 64-----------------------------\n",
      "loss: 3.485517578125\n",
      "loss: 1785.2177100219726\n",
      "loss: 1804.0734340820313\n",
      "loss: 1808.6470845947265\n",
      "Avg loss: 1849.418212890625!\n",
      "Epoch 65-----------------------------\n",
      "loss: 3.47440576171875\n",
      "loss: 1780.2732798461914\n",
      "loss: 1799.0791109313964\n",
      "loss: 1803.318518737793\n",
      "Avg loss: 1844.462646484375!\n",
      "Epoch 66-----------------------------\n",
      "loss: 3.462261474609375\n",
      "loss: 1775.195551574707\n",
      "loss: 1794.3163385314942\n",
      "loss: 1798.3540789489746\n",
      "Avg loss: 1839.708984375!\n",
      "Epoch 67-----------------------------\n",
      "loss: 3.45176220703125\n",
      "loss: 1770.2763290405273\n",
      "loss: 1789.7670299072265\n",
      "loss: 1793.4989494934082\n",
      "Avg loss: 1835.2337646484375!\n",
      "Epoch 68-----------------------------\n",
      "loss: 3.443121826171875\n",
      "loss: 1765.603214691162\n",
      "loss: 1785.1573441467285\n",
      "loss: 1788.5993671569825\n",
      "Avg loss: 1830.7003173828125!\n",
      "Epoch 69-----------------------------\n",
      "loss: 3.434498291015625\n",
      "loss: 1760.8668602905273\n",
      "loss: 1780.7244815368651\n",
      "loss: 1783.8770514831542\n",
      "Avg loss: 1826.2947998046875!\n",
      "Epoch 70-----------------------------\n",
      "loss: 3.427158203125\n",
      "loss: 1756.237271850586\n",
      "loss: 1776.415337310791\n",
      "loss: 1779.2141175231934\n",
      "Avg loss: 1822.074951171875!\n",
      "Epoch 71-----------------------------\n",
      "loss: 3.41703564453125\n",
      "loss: 1751.7990134277343\n",
      "loss: 1772.232316253662\n",
      "loss: 1774.7786322631837\n",
      "Avg loss: 1817.945068359375!\n",
      "Epoch 72-----------------------------\n",
      "loss: 3.40574169921875\n",
      "loss: 1742.7617214660645\n",
      "loss: 1714.9275711517334\n",
      "loss: 1544.6077318496705\n",
      "Avg loss: 1478.719482421875!\n",
      "Epoch 73-----------------------------\n",
      "loss: 3.003568115234375\n",
      "loss: 1383.7159271202088\n",
      "loss: 1355.9472421150208\n",
      "loss: 1352.0413691864014\n",
      "Avg loss: 1383.2750244140625!\n",
      "Epoch 74-----------------------------\n",
      "loss: 2.898947509765625\n",
      "loss: 1313.8034374465942\n",
      "loss: 1304.620317565918\n",
      "loss: 1320.8674931106568\n",
      "Avg loss: 1360.4248046875!\n",
      "Epoch 75-----------------------------\n",
      "loss: 2.89713916015625\n",
      "loss: 1291.8499024658204\n",
      "loss: 1282.3792698440552\n",
      "loss: 1302.052902809143\n",
      "Avg loss: 1343.8228759765625!\n",
      "Epoch 76-----------------------------\n",
      "loss: 2.88464013671875\n",
      "loss: 1275.390314086914\n",
      "loss: 1266.8413264389037\n",
      "loss: 1288.022056678772\n",
      "Avg loss: 1330.8255615234375!\n",
      "Epoch 77-----------------------------\n",
      "loss: 2.87069775390625\n",
      "loss: 1262.004673942566\n",
      "loss: 1253.8440793685913\n",
      "loss: 1275.728789527893\n",
      "Avg loss: 1319.3759765625!\n",
      "Epoch 78-----------------------------\n",
      "loss: 2.8555859375\n",
      "loss: 1250.3863481903077\n",
      "loss: 1242.9547037200928\n",
      "loss: 1265.251607963562\n",
      "Avg loss: 1309.5797119140625!\n",
      "Epoch 79-----------------------------\n",
      "loss: 2.844681640625\n",
      "loss: 1240.4038052368164\n",
      "loss: 1233.579861503601\n",
      "loss: 1255.9772850646973\n",
      "Avg loss: 1300.8426513671875!\n",
      "Epoch 80-----------------------------\n",
      "loss: 2.8344970703125\n",
      "loss: 1231.5009823989867\n",
      "loss: 1225.2835379791259\n",
      "loss: 1247.6030043678284\n",
      "Avg loss: 1292.7523193359375!\n",
      "Epoch 81-----------------------------\n",
      "loss: 2.82750048828125\n",
      "loss: 1223.008124420166\n",
      "loss: 1216.338104347229\n",
      "loss: 1238.34155178833\n",
      "Avg loss: 1284.0487060546875!\n",
      "Epoch 82-----------------------------\n",
      "loss: 2.822611083984375\n",
      "loss: 1214.4615390396118\n",
      "loss: 1209.1550104026794\n",
      "loss: 1231.0391943130494\n",
      "Avg loss: 1277.013671875!\n",
      "Epoch 83-----------------------------\n",
      "loss: 2.81392626953125\n",
      "loss: 1207.329846862793\n",
      "loss: 1202.5826245803833\n",
      "loss: 1224.2210689735412\n",
      "Avg loss: 1270.275390625!\n",
      "Epoch 84-----------------------------\n",
      "loss: 2.80495361328125\n",
      "loss: 1200.58711775589\n",
      "loss: 1196.1926673431396\n",
      "loss: 1217.7578875350953\n",
      "Avg loss: 1264.1343994140625!\n",
      "Epoch 85-----------------------------\n",
      "loss: 2.797456298828125\n",
      "loss: 1194.2692090835571\n",
      "loss: 1190.350274307251\n",
      "loss: 1211.6204303283691\n",
      "Avg loss: 1258.2347412109375!\n",
      "Epoch 86-----------------------------\n",
      "loss: 2.79150390625\n",
      "loss: 1188.254601257324\n",
      "loss: 1184.5473325309754\n",
      "loss: 1205.7907839736938\n",
      "Avg loss: 1252.6363525390625!\n",
      "Epoch 87-----------------------------\n",
      "loss: 2.784935791015625\n",
      "loss: 1182.5726615447998\n",
      "loss: 1179.238396461487\n",
      "loss: 1200.2823632659913\n",
      "Avg loss: 1247.4954833984375!\n",
      "Epoch 88-----------------------------\n",
      "loss: 2.779853515625\n",
      "loss: 1177.2457816963197\n",
      "loss: 1174.1634600524903\n",
      "loss: 1194.8481483192443\n",
      "Avg loss: 1242.2882080078125!\n",
      "Epoch 89-----------------------------\n",
      "loss: 2.774997802734375\n",
      "loss: 1171.98717272377\n",
      "loss: 1168.7201250228882\n",
      "loss: 1189.3019059867859\n",
      "Avg loss: 1236.8717041015625!\n",
      "Epoch 90-----------------------------\n",
      "loss: 2.77005078125\n",
      "loss: 1166.4658854866027\n",
      "loss: 1163.9530798912049\n",
      "loss: 1184.3759312477112\n",
      "Avg loss: 1232.180908203125!\n",
      "Epoch 91-----------------------------\n",
      "loss: 2.764896240234375\n",
      "loss: 1161.6460740623475\n",
      "loss: 1159.3625287857055\n",
      "loss: 1179.49643857193\n",
      "Avg loss: 1227.5230712890625!\n",
      "Epoch 92-----------------------------\n",
      "loss: 2.760521484375\n",
      "loss: 1156.8215132522582\n",
      "loss: 1154.454854320526\n",
      "loss: 1174.597652004242\n",
      "Avg loss: 1222.78564453125!\n",
      "Epoch 93-----------------------------\n",
      "loss: 2.75797998046875\n",
      "loss: 1151.9692462062835\n",
      "loss: 1149.8582382965087\n",
      "loss: 1169.9172272701264\n",
      "Avg loss: 1218.4466552734375!\n",
      "Epoch 94-----------------------------\n",
      "loss: 2.75774658203125\n",
      "loss: 1147.5112565345764\n",
      "loss: 1145.6328401832582\n",
      "loss: 1165.528571516037\n",
      "Avg loss: 1214.0811767578125!\n",
      "Epoch 95-----------------------------\n",
      "loss: 2.754033447265625\n",
      "loss: 1143.026124162674\n",
      "loss: 1141.3351967010499\n",
      "loss: 1161.1500731639862\n",
      "Avg loss: 1210.0626220703125!\n",
      "Epoch 96-----------------------------\n",
      "loss: 2.75016796875\n",
      "loss: 1138.741865966797\n",
      "loss: 1137.2117431411743\n",
      "loss: 1156.9556566543579\n",
      "Avg loss: 1205.984619140625!\n",
      "Epoch 97-----------------------------\n",
      "loss: 2.745022705078125\n",
      "loss: 1134.556499660492\n",
      "loss: 1132.824815776825\n",
      "loss: 1152.35500450325\n",
      "Avg loss: 1201.6844482421875!\n",
      "Epoch 98-----------------------------\n",
      "loss: 2.739892578125\n",
      "loss: 1130.1202410068513\n",
      "loss: 1129.1862420310974\n",
      "loss: 1148.5480274276733\n",
      "Avg loss: 1197.82958984375!\n",
      "Epoch 99-----------------------------\n",
      "loss: 2.731556884765625\n",
      "loss: 1126.2623573303222\n",
      "loss: 1125.5408189163209\n",
      "loss: 1144.7700809268952\n",
      "Avg loss: 1194.460693359375!\n",
      "Epoch 100-----------------------------\n",
      "loss: 2.724382568359375\n",
      "loss: 1122.7180138530732\n",
      "loss: 1121.4084318733214\n",
      "loss: 1140.6198746700286\n",
      "Avg loss: 1191.1611328125!\n",
      "Epoch 101-----------------------------\n",
      "loss: 2.716227783203125\n",
      "loss: 1119.2670648765563\n",
      "loss: 1118.7988958625795\n",
      "loss: 1137.687111673355\n",
      "Avg loss: 1187.638916015625!\n",
      "Epoch 102-----------------------------\n",
      "loss: 2.70997998046875\n",
      "loss: 1115.763152908325\n",
      "loss: 1114.8884960479736\n",
      "loss: 1133.6249333953858\n",
      "Avg loss: 1184.3616943359375!\n",
      "Epoch 103-----------------------------\n",
      "loss: 2.703114013671875\n",
      "loss: 1112.3701483726502\n",
      "loss: 1112.4270701656342\n",
      "loss: 1130.9032438488007\n",
      "Avg loss: 1181.1170654296875!\n",
      "Epoch 104-----------------------------\n",
      "loss: 2.69764794921875\n",
      "loss: 1109.136806684494\n",
      "loss: 1108.7413647403716\n",
      "loss: 1127.133969936371\n",
      "Avg loss: 1178.07763671875!\n",
      "Epoch 105-----------------------------\n",
      "loss: 2.689303466796875\n",
      "loss: 1106.1943224411011\n",
      "loss: 1106.4816456050874\n",
      "loss: 1124.7638288230896\n",
      "Avg loss: 1175.2288818359375!\n",
      "Epoch 106-----------------------------\n",
      "loss: 2.67978955078125\n",
      "loss: 1103.1159544467926\n",
      "loss: 1103.677458360672\n",
      "loss: 1121.5886765651703\n",
      "Avg loss: 1172.18603515625!\n",
      "Epoch 107-----------------------------\n",
      "loss: 2.670322021484375\n",
      "loss: 1100.1888344707488\n",
      "loss: 1100.9601044540404\n",
      "loss: 1118.9151351528167\n",
      "Avg loss: 1170.2548828125!\n",
      "Epoch 108-----------------------------\n",
      "loss: 2.659541259765625\n",
      "loss: 1098.2358085918427\n",
      "loss: 1098.564370819092\n",
      "loss: 1116.3230494747163\n",
      "Avg loss: 1167.6302490234375!\n",
      "Epoch 109-----------------------------\n",
      "loss: 2.650242919921875\n",
      "loss: 1095.5503572330474\n",
      "loss: 1096.2030795288085\n",
      "loss: 1113.5475820503234\n",
      "Avg loss: 1165.107421875!\n",
      "Epoch 110-----------------------------\n",
      "loss: 2.642008544921875\n",
      "loss: 1092.9941588191987\n",
      "loss: 1094.20482654953\n",
      "loss: 1111.6017072467805\n",
      "Avg loss: 1162.6424560546875!\n",
      "Epoch 111-----------------------------\n",
      "loss: 2.632384521484375\n",
      "loss: 1090.4584701404572\n",
      "loss: 1091.9859946460724\n",
      "loss: 1109.1760663814546\n",
      "Avg loss: 1160.8377685546875!\n",
      "Epoch 112-----------------------------\n",
      "loss: 2.62034228515625\n",
      "loss: 1088.7062965717316\n",
      "loss: 1089.7798821868896\n",
      "loss: 1106.6369751434327\n",
      "Avg loss: 1158.7698974609375!\n",
      "Epoch 113-----------------------------\n",
      "loss: 2.611620849609375\n",
      "loss: 1086.7026321811677\n",
      "loss: 1088.2679855613708\n",
      "loss: 1105.1054318466186\n",
      "Avg loss: 1156.643310546875!\n",
      "Epoch 114-----------------------------\n",
      "loss: 2.602657470703125\n",
      "loss: 1084.515635576248\n",
      "loss: 1086.4438145503998\n",
      "loss: 1103.1036291599273\n",
      "Avg loss: 1155.427978515625!\n",
      "Epoch 115-----------------------------\n",
      "loss: 2.5912978515625\n",
      "loss: 1083.3339947681427\n",
      "loss: 1084.7183920555115\n",
      "loss: 1101.1594402904511\n",
      "Avg loss: 1153.343017578125!\n",
      "Epoch 116-----------------------------\n",
      "loss: 2.58119775390625\n",
      "loss: 1081.269810049057\n",
      "loss: 1082.8830084838867\n",
      "loss: 1098.983158016205\n",
      "Avg loss: 1151.54443359375!\n",
      "Epoch 117-----------------------------\n",
      "loss: 2.570849853515625\n",
      "loss: 1079.6733553218842\n",
      "loss: 1081.937209749222\n",
      "loss: 1097.8664933242799\n",
      "Avg loss: 1149.8758544921875!\n",
      "Epoch 118-----------------------------\n",
      "loss: 2.56092236328125\n",
      "loss: 1077.902451007843\n",
      "loss: 1080.3582445316315\n",
      "loss: 1096.2121153907776\n",
      "Avg loss: 1149.010986328125!\n",
      "Epoch 119-----------------------------\n",
      "loss: 2.547440673828125\n",
      "loss: 1077.0806174964905\n",
      "loss: 1078.978191789627\n",
      "loss: 1094.5879130859375\n",
      "Avg loss: 1147.1923828125!\n",
      "Epoch 120-----------------------------\n",
      "loss: 2.5352900390625\n",
      "loss: 1075.5138190727234\n",
      "loss: 1077.8107411727906\n",
      "loss: 1092.9643441619874\n",
      "Avg loss: 1145.7835693359375!\n",
      "Epoch 121-----------------------------\n",
      "loss: 2.522770263671875\n",
      "loss: 1074.1821962738038\n",
      "loss: 1076.5613865661621\n",
      "loss: 1091.5975901031495\n",
      "Avg loss: 1144.498779296875!\n",
      "Epoch 122-----------------------------\n",
      "loss: 2.510111083984375\n",
      "loss: 1072.929074596405\n",
      "loss: 1075.5134465274812\n",
      "loss: 1090.4527017917633\n",
      "Avg loss: 1143.4039306640625!\n",
      "Epoch 123-----------------------------\n",
      "loss: 2.497347412109375\n",
      "loss: 1071.876255039215\n",
      "loss: 1074.5983184280396\n",
      "loss: 1089.3496097221375\n",
      "Avg loss: 1142.3873291015625!\n",
      "Epoch 124-----------------------------\n",
      "loss: 2.484103271484375\n",
      "loss: 1070.915200744629\n",
      "loss: 1074.1230167350768\n",
      "loss: 1088.7109096050262\n",
      "Avg loss: 1141.53662109375!\n",
      "Epoch 125-----------------------------\n",
      "loss: 2.470382080078125\n",
      "loss: 1070.2106973896027\n",
      "loss: 1072.9250099067688\n",
      "loss: 1087.1249856510162\n",
      "Avg loss: 1140.298828125!\n",
      "Epoch 126-----------------------------\n",
      "loss: 2.459077880859375\n",
      "loss: 1069.1372247714996\n",
      "loss: 1072.1003731651306\n",
      "loss: 1086.09514336586\n",
      "Avg loss: 1139.3033447265625!\n",
      "Epoch 127-----------------------------\n",
      "loss: 2.44751171875\n",
      "loss: 1068.1796396713257\n",
      "loss: 1071.2621409835815\n",
      "loss: 1085.1373263530731\n",
      "Avg loss: 1138.50537109375!\n",
      "Epoch 128-----------------------------\n",
      "loss: 2.431541015625\n",
      "loss: 1067.4134476490021\n",
      "loss: 1070.565124578476\n",
      "loss: 1084.1713348407745\n",
      "Avg loss: 1137.48193359375!\n",
      "Epoch 129-----------------------------\n",
      "loss: 2.418321533203125\n",
      "loss: 1066.5266055641175\n",
      "loss: 1069.9641572113037\n",
      "loss: 1083.3186620140075\n",
      "Avg loss: 1136.408203125!\n",
      "Epoch 130-----------------------------\n",
      "loss: 2.40500439453125\n",
      "loss: 1065.576152021408\n",
      "loss: 1069.1533936862945\n",
      "loss: 1082.0497401123048\n",
      "Avg loss: 1135.45703125!\n",
      "Epoch 131-----------------------------\n",
      "loss: 2.39075634765625\n",
      "loss: 1064.8237263412475\n",
      "loss: 1069.0098603477477\n",
      "loss: 1081.7248293476105\n",
      "Avg loss: 1135.2066650390625!\n",
      "Epoch 132-----------------------------\n",
      "loss: 2.376458251953125\n",
      "loss: 1064.651096956253\n",
      "loss: 1068.2948517894745\n",
      "loss: 1080.7878489837647\n",
      "Avg loss: 1134.4752197265625!\n",
      "Epoch 133-----------------------------\n",
      "loss: 2.36533251953125\n",
      "loss: 1063.9822607192993\n",
      "loss: 1067.8027826728821\n",
      "loss: 1080.0254757156372\n",
      "Avg loss: 1133.7552490234375!\n",
      "Epoch 134-----------------------------\n",
      "loss: 2.351845947265625\n",
      "loss: 1063.3483098983766\n",
      "loss: 1067.380665309906\n",
      "loss: 1079.3673653926849\n",
      "Avg loss: 1133.1949462890625!\n",
      "Epoch 135-----------------------------\n",
      "loss: 2.337738525390625\n",
      "loss: 1062.812250026703\n",
      "loss: 1066.928888814926\n",
      "loss: 1078.5775870037078\n",
      "Avg loss: 1132.0548095703125!\n",
      "Epoch 136-----------------------------\n",
      "loss: 2.324348388671875\n",
      "loss: 1061.9779105491639\n",
      "loss: 1066.8288730888366\n",
      "loss: 1077.9579392757416\n",
      "Avg loss: 1131.6942138671875!\n",
      "Epoch 137-----------------------------\n",
      "loss: 2.3110693359375\n",
      "loss: 1061.804906621933\n",
      "loss: 1066.1127821884156\n",
      "loss: 1076.9839046611785\n",
      "Avg loss: 1130.8394775390625!\n",
      "Epoch 138-----------------------------\n",
      "loss: 2.298753662109375\n",
      "loss: 1061.0205859832763\n",
      "loss: 1065.5827901325226\n",
      "loss: 1076.2263717803955\n",
      "Avg loss: 1130.2159423828125!\n",
      "Epoch 139-----------------------------\n",
      "loss: 2.283737060546875\n",
      "loss: 1060.529913471222\n",
      "loss: 1065.726777065277\n",
      "loss: 1076.0882557735442\n",
      "Avg loss: 1130.00341796875!\n",
      "Epoch 140-----------------------------\n",
      "loss: 2.267705810546875\n",
      "loss: 1060.4783086090088\n",
      "loss: 1065.1779642734527\n",
      "loss: 1075.327464904785\n",
      "Avg loss: 1129.3905029296875!\n",
      "Epoch 141-----------------------------\n",
      "loss: 2.253802978515625\n",
      "loss: 1060.0094010696412\n",
      "loss: 1065.4115362987518\n",
      "loss: 1075.3360897197724\n",
      "Avg loss: 1129.440185546875!\n",
      "Epoch 142-----------------------------\n",
      "loss: 2.23720849609375\n",
      "loss: 1060.172069929123\n",
      "loss: 1065.0042100811004\n",
      "loss: 1074.561730424881\n",
      "Avg loss: 1128.3642578125!\n",
      "Epoch 143-----------------------------\n",
      "loss: 2.22308642578125\n",
      "loss: 1059.3518644771575\n",
      "loss: 1064.5217887763977\n",
      "loss: 1073.3841767482756\n",
      "Avg loss: 1127.512451171875!\n",
      "Epoch 144-----------------------------\n",
      "loss: 2.21081787109375\n",
      "loss: 1058.8001477241517\n",
      "loss: 1064.6844970703125\n",
      "loss: 1073.3675175552369\n",
      "Avg loss: 1127.5523681640625!\n",
      "Epoch 145-----------------------------\n",
      "loss: 2.196491943359375\n",
      "loss: 1058.9269580745697\n",
      "loss: 1064.2287122974396\n",
      "loss: 1072.6002652301788\n",
      "Avg loss: 1126.8992919921875!\n",
      "Epoch 146-----------------------------\n",
      "loss: 2.183042236328125\n",
      "loss: 1058.4454575309753\n",
      "loss: 1064.0234467372895\n",
      "loss: 1071.9492329788209\n",
      "Avg loss: 1126.3092041015625!\n",
      "Epoch 147-----------------------------\n",
      "loss: 2.17028076171875\n",
      "loss: 1057.9671312961577\n",
      "loss: 1064.189028989792\n",
      "loss: 1071.927588394165\n",
      "Avg loss: 1126.4093017578125!\n",
      "Epoch 148-----------------------------\n",
      "loss: 2.155794921875\n",
      "loss: 1058.264905611038\n",
      "loss: 1063.982383146286\n",
      "loss: 1071.2919195919037\n",
      "Avg loss: 1125.820556640625!\n",
      "Epoch 149-----------------------------\n",
      "loss: 2.143283935546875\n",
      "loss: 1057.8438161125182\n",
      "loss: 1064.144681388855\n",
      "loss: 1071.2809007034302\n",
      "Avg loss: 1125.803955078125!\n",
      "Epoch 150-----------------------------\n",
      "loss: 2.129423095703125\n",
      "loss: 1058.0383408851624\n",
      "loss: 1063.8293413028716\n",
      "loss: 1070.441556695938\n",
      "Avg loss: 1124.6417236328125!\n",
      "Epoch 151-----------------------------\n",
      "loss: 2.116817138671875\n",
      "loss: 1057.096790090561\n",
      "loss: 1063.6711722869873\n",
      "loss: 1069.7432133312225\n",
      "Avg loss: 1124.2164306640625!\n",
      "Epoch 152-----------------------------\n",
      "loss: 2.10400048828125\n",
      "loss: 1056.9943667621612\n",
      "loss: 1063.106659685135\n",
      "loss: 1068.7765958576204\n",
      "Avg loss: 1123.3499755859375!\n",
      "Epoch 153-----------------------------\n",
      "loss: 2.092444091796875\n",
      "loss: 1056.299135219574\n",
      "loss: 1063.1171971035003\n",
      "loss: 1068.60794947052\n",
      "Avg loss: 1123.14599609375!\n",
      "Epoch 154-----------------------------\n",
      "loss: 2.078193359375\n",
      "loss: 1056.3674074649812\n",
      "loss: 1062.7270313892363\n",
      "loss: 1067.7003648166656\n",
      "Avg loss: 1122.2593994140625!\n",
      "Epoch 155-----------------------------\n",
      "loss: 2.066792724609375\n",
      "loss: 1055.5855045280457\n",
      "loss: 1062.1502094364166\n",
      "loss: 1066.9562634277345\n",
      "Avg loss: 1121.6700439453125!\n",
      "Epoch 156-----------------------------\n",
      "loss: 2.0549453125\n",
      "loss: 1055.092622171402\n",
      "loss: 1062.4568699531555\n",
      "loss: 1066.8192580566406\n",
      "Avg loss: 1121.6202392578125!\n",
      "Epoch 157-----------------------------\n",
      "loss: 2.038785888671875\n",
      "loss: 1055.225955892563\n",
      "loss: 1062.4427305927277\n",
      "loss: 1066.5091630096435\n",
      "Avg loss: 1121.2493896484375!\n",
      "Epoch 158-----------------------------\n",
      "loss: 2.0261151123046877\n",
      "loss: 1055.0970740909577\n",
      "loss: 1061.815004076004\n",
      "loss: 1065.6383691520691\n",
      "Avg loss: 1120.540771484375!\n",
      "Epoch 159-----------------------------\n",
      "loss: 2.01342431640625\n",
      "loss: 1054.610765083313\n",
      "loss: 1061.9502975845337\n",
      "loss: 1065.3699905223846\n",
      "Avg loss: 1120.1875!\n",
      "Epoch 160-----------------------------\n",
      "loss: 2.0020233154296876\n",
      "loss: 1054.4396568260192\n",
      "loss: 1061.1214182300569\n",
      "loss: 1064.2276498565675\n",
      "Avg loss: 1118.6893310546875!\n",
      "Epoch 161-----------------------------\n",
      "loss: 1.9912476806640624\n",
      "loss: 1053.193209098816\n",
      "loss: 1060.7850528469085\n",
      "loss: 1063.094863548279\n",
      "Avg loss: 1117.8729248046875!\n",
      "Epoch 162-----------------------------\n",
      "loss: 1.98042431640625\n",
      "loss: 1052.6998831043243\n",
      "loss: 1060.3356504325866\n",
      "loss: 1062.5137515220642\n",
      "Avg loss: 1117.5653076171875!\n",
      "Epoch 163-----------------------------\n",
      "loss: 1.967832763671875\n",
      "loss: 1052.4303580474852\n",
      "loss: 1059.5903933620452\n",
      "loss: 1061.279877040863\n",
      "Avg loss: 1116.4854736328125!\n",
      "Epoch 164-----------------------------\n",
      "loss: 1.9552305908203125\n",
      "loss: 1051.4311805152893\n",
      "loss: 1059.3202065887451\n",
      "loss: 1060.8077015953063\n",
      "Avg loss: 1116.2745361328125!\n",
      "Epoch 165-----------------------------\n",
      "loss: 1.941822509765625\n",
      "loss: 1051.4013726348876\n",
      "loss: 1059.1781194782257\n",
      "loss: 1060.3284557075501\n",
      "Avg loss: 1116.0469970703125!\n",
      "Epoch 166-----------------------------\n",
      "loss: 1.9270914306640625\n",
      "loss: 1051.2741520290374\n",
      "loss: 1058.7547023983002\n",
      "loss: 1059.5107119312286\n",
      "Avg loss: 1115.0706787109375!\n",
      "Epoch 167-----------------------------\n",
      "loss: 1.913930908203125\n",
      "loss: 1050.5565445671082\n",
      "loss: 1058.5203313064576\n",
      "loss: 1059.0744493083953\n",
      "Avg loss: 1114.8448486328125!\n",
      "Epoch 168-----------------------------\n",
      "loss: 1.899138427734375\n",
      "loss: 1050.5149105548858\n",
      "loss: 1058.173496963501\n",
      "loss: 1058.2557439727784\n",
      "Avg loss: 1114.00341796875!\n",
      "Epoch 169-----------------------------\n",
      "loss: 1.8855968017578124\n",
      "loss: 1049.9417468156814\n",
      "loss: 1058.3473836898804\n",
      "loss: 1058.1158593788148\n",
      "Avg loss: 1113.48095703125!\n",
      "Epoch 170-----------------------------\n",
      "loss: 1.86962744140625\n",
      "loss: 1049.7784559326171\n",
      "loss: 1057.5354585990906\n",
      "loss: 1056.21779804039\n",
      "Avg loss: 1112.061767578125!\n",
      "Epoch 171-----------------------------\n",
      "loss: 1.8586778564453126\n",
      "loss: 1048.6532875843047\n",
      "loss: 1057.5272110691071\n",
      "loss: 1056.0773492717742\n",
      "Avg loss: 1111.894287109375!\n",
      "Epoch 172-----------------------------\n",
      "loss: 1.8433428955078126\n",
      "loss: 1048.8428111896515\n",
      "loss: 1057.751929365158\n",
      "loss: 1055.9528283576965\n",
      "Avg loss: 1111.837158203125!\n",
      "Epoch 173-----------------------------\n",
      "loss: 1.82878369140625\n",
      "loss: 1048.9903654537202\n",
      "loss: 1057.3601075763702\n",
      "loss: 1054.9461592559815\n",
      "Avg loss: 1110.89111328125!\n",
      "Epoch 174-----------------------------\n",
      "loss: 1.8160595703125\n",
      "loss: 1048.2045212640762\n",
      "loss: 1056.9923469409944\n",
      "loss: 1054.469738828659\n",
      "Avg loss: 1110.718505859375!\n",
      "Epoch 175-----------------------------\n",
      "loss: 1.8022410888671876\n",
      "loss: 1048.1141239299775\n",
      "loss: 1056.6083039417267\n",
      "loss: 1053.638997724533\n",
      "Avg loss: 1110.026123046875!\n",
      "Epoch 176-----------------------------\n",
      "loss: 1.7877554931640625\n",
      "loss: 1047.5817264232635\n",
      "loss: 1056.87671475029\n",
      "loss: 1053.4680625724793\n",
      "Avg loss: 1109.7908935546875!\n",
      "Epoch 177-----------------------------\n",
      "loss: 1.7733909912109376\n",
      "loss: 1047.5431647109986\n",
      "loss: 1056.16371692276\n",
      "loss: 1052.4208535881041\n",
      "Avg loss: 1108.9776611328125!\n",
      "Epoch 178-----------------------------\n",
      "loss: 1.7605123291015625\n",
      "loss: 1046.8515365781784\n",
      "loss: 1056.3308896827698\n",
      "loss: 1052.2576389427186\n",
      "Avg loss: 1108.840576171875!\n",
      "Epoch 179-----------------------------\n",
      "loss: 1.7462205810546876\n",
      "loss: 1046.8919036951065\n",
      "loss: 1056.35716737175\n",
      "loss: 1051.9046278152466\n",
      "Avg loss: 1108.5948486328125!\n",
      "Epoch 180-----------------------------\n",
      "loss: 1.7321903076171874\n",
      "loss: 1046.7677926273345\n",
      "loss: 1055.644220779419\n",
      "loss: 1050.5927625312804\n",
      "Avg loss: 1107.072265625!\n",
      "Epoch 181-----------------------------\n",
      "loss: 1.7200797119140625\n",
      "loss: 1045.6914695482253\n",
      "loss: 1055.671438533783\n",
      "loss: 1049.7631322727204\n",
      "Avg loss: 1106.630859375!\n",
      "Epoch 182-----------------------------\n",
      "loss: 1.706046142578125\n",
      "loss: 1045.4469017791748\n",
      "loss: 1054.8548289413452\n",
      "loss: 1048.6156245670318\n",
      "Avg loss: 1105.83544921875!\n",
      "Epoch 183-----------------------------\n",
      "loss: 1.6940875244140625\n",
      "loss: 1044.7016701669693\n",
      "loss: 1055.0406185493468\n",
      "loss: 1048.4554705867768\n",
      "Avg loss: 1105.7613525390625!\n",
      "Epoch 184-----------------------------\n",
      "loss: 1.6806920166015624\n",
      "loss: 1044.7827430143357\n",
      "loss: 1054.764894054413\n",
      "loss: 1047.7488472995758\n",
      "Avg loss: 1105.33544921875!\n",
      "Epoch 185-----------------------------\n",
      "loss: 1.6676253662109375\n",
      "loss: 1044.3965493173598\n",
      "loss: 1054.1662631950378\n",
      "loss: 1046.518002216339\n",
      "Avg loss: 1104.219970703125!\n",
      "Epoch 186-----------------------------\n",
      "loss: 1.6550948486328125\n",
      "loss: 1043.4175153141023\n",
      "loss: 1054.014724697113\n",
      "loss: 1046.1073191280366\n",
      "Avg loss: 1103.97802734375!\n",
      "Epoch 187-----------------------------\n",
      "loss: 1.6412542724609376\n",
      "loss: 1043.3050142621994\n",
      "loss: 1053.3260826129913\n",
      "loss: 1044.960427427292\n",
      "Avg loss: 1103.060546875!\n",
      "Epoch 188-----------------------------\n",
      "loss: 1.6296102294921875\n",
      "loss: 1042.6338054237365\n",
      "loss: 1053.517973537445\n",
      "loss: 1044.7466405220032\n",
      "Avg loss: 1102.8359375!\n",
      "Epoch 189-----------------------------\n",
      "loss: 1.61649658203125\n",
      "loss: 1042.436166563034\n",
      "loss: 1052.6011154689788\n",
      "loss: 1043.4657599010468\n",
      "Avg loss: 1101.9947509765625!\n",
      "Epoch 190-----------------------------\n",
      "loss: 1.6011748046875\n",
      "loss: 1041.6168187236785\n",
      "loss: 1052.864759027481\n",
      "loss: 1043.2850461158753\n",
      "Avg loss: 1101.920166015625!\n",
      "Epoch 191-----------------------------\n",
      "loss: 1.586661376953125\n",
      "loss: 1041.819506679535\n",
      "loss: 1053.038682954788\n",
      "loss: 1042.703148677826\n",
      "Avg loss: 1101.6055908203125!\n",
      "Epoch 192-----------------------------\n",
      "loss: 1.5690269775390624\n",
      "loss: 1041.5236277770996\n",
      "loss: 1052.2355090694427\n",
      "loss: 1041.5191114997863\n",
      "Avg loss: 1100.6365966796875!\n",
      "Epoch 193-----------------------------\n",
      "loss: 1.5540994873046876\n",
      "loss: 1040.8085819568635\n",
      "loss: 1052.2944669647218\n",
      "loss: 1041.0838471374511\n",
      "Avg loss: 1100.37646484375!\n",
      "Epoch 194-----------------------------\n",
      "loss: 1.54021826171875\n",
      "loss: 1040.8799270896911\n",
      "loss: 1052.3011528110503\n",
      "loss: 1040.5364237747192\n",
      "Avg loss: 1099.8843994140625!\n",
      "Epoch 195-----------------------------\n",
      "loss: 1.5275848388671875\n",
      "loss: 1040.5687028427124\n",
      "loss: 1051.4560285549164\n",
      "loss: 1039.047377948761\n",
      "Avg loss: 1098.697998046875!\n",
      "Epoch 196-----------------------------\n",
      "loss: 1.5147945556640625\n",
      "loss: 1039.4932246112824\n",
      "loss: 1051.1765424842833\n",
      "loss: 1038.6383795814513\n",
      "Avg loss: 1098.1290283203125!\n",
      "Epoch 197-----------------------------\n",
      "loss: 1.5033582763671876\n",
      "loss: 1039.2751038131714\n",
      "loss: 1050.985644870758\n",
      "loss: 1037.9591465377807\n",
      "Avg loss: 1097.536376953125!\n",
      "Epoch 198-----------------------------\n",
      "loss: 1.492788330078125\n",
      "loss: 1038.6921445732116\n",
      "loss: 1049.8302877140045\n",
      "loss: 1036.2304124412537\n",
      "Avg loss: 1096.0125732421875!\n",
      "Epoch 199-----------------------------\n",
      "loss: 1.482088134765625\n",
      "loss: 1037.6281129550935\n",
      "loss: 1049.3528562755585\n",
      "loss: 1035.5609655036926\n",
      "Avg loss: 1095.3258056640625!\n",
      "Epoch 200-----------------------------\n",
      "loss: 1.4727586669921875\n",
      "loss: 1037.0865271282196\n",
      "loss: 1049.4222589950562\n",
      "loss: 1035.4653852901458\n",
      "Avg loss: 1095.355712890625!\n",
      "Epoch 201-----------------------------\n",
      "loss: 1.4611287841796874\n",
      "loss: 1037.1946709785461\n",
      "loss: 1048.9115064888001\n",
      "loss: 1034.2070642223357\n",
      "Avg loss: 1094.030517578125!\n",
      "Epoch 202-----------------------------\n",
      "loss: 1.44923193359375\n",
      "loss: 1036.047471633911\n",
      "loss: 1048.769412340164\n",
      "loss: 1033.2707917404175\n",
      "Avg loss: 1093.4365234375!\n",
      "Epoch 203-----------------------------\n",
      "loss: 1.4378311767578125\n",
      "loss: 1035.7213685913086\n",
      "loss: 1047.8852868537904\n",
      "loss: 1032.0854100074769\n",
      "Avg loss: 1092.466552734375!\n",
      "Epoch 204-----------------------------\n",
      "loss: 1.428427490234375\n",
      "loss: 1034.8331017284393\n",
      "loss: 1047.358922908783\n",
      "loss: 1031.2254596977234\n",
      "Avg loss: 1091.853759765625!\n",
      "Epoch 205-----------------------------\n",
      "loss: 1.419142822265625\n",
      "loss: 1034.359569290161\n",
      "loss: 1047.408317111969\n",
      "loss: 1030.9446022758484\n",
      "Avg loss: 1091.47802734375!\n",
      "Epoch 206-----------------------------\n",
      "loss: 1.40862841796875\n",
      "loss: 1034.1736801490783\n",
      "loss: 1046.5915155906678\n",
      "loss: 1029.8237808322906\n",
      "Avg loss: 1090.5291748046875!\n",
      "Epoch 207-----------------------------\n",
      "loss: 1.4006995849609376\n",
      "loss: 1033.3287812023163\n",
      "loss: 1046.5385191078185\n",
      "loss: 1029.3944284057618\n",
      "Avg loss: 1090.322021484375!\n",
      "Epoch 208-----------------------------\n",
      "loss: 1.39005224609375\n",
      "loss: 1033.1767123985292\n",
      "loss: 1045.8091965675353\n",
      "loss: 1028.375064523697\n",
      "Avg loss: 1089.296630859375!\n",
      "Epoch 209-----------------------------\n",
      "loss: 1.382255126953125\n",
      "loss: 1032.2707070789338\n",
      "loss: 1045.7130884838105\n",
      "loss: 1027.9057289276122\n",
      "Avg loss: 1089.0577392578125!\n",
      "Epoch 210-----------------------------\n",
      "loss: 1.372317626953125\n",
      "loss: 1032.1643381214142\n",
      "loss: 1045.6357813510895\n",
      "loss: 1027.5281662139892\n",
      "Avg loss: 1088.6727294921875!\n",
      "Epoch 211-----------------------------\n",
      "loss: 1.36441015625\n",
      "loss: 1031.892994588852\n",
      "loss: 1044.7316729927063\n",
      "loss: 1026.479458656311\n",
      "Avg loss: 1087.9053955078125!\n",
      "Epoch 212-----------------------------\n",
      "loss: 1.35882080078125\n",
      "loss: 1031.1212715358733\n",
      "loss: 1044.8110318870545\n",
      "loss: 1026.1470434188843\n",
      "Avg loss: 1087.804931640625!\n",
      "Epoch 213-----------------------------\n",
      "loss: 1.3503603515625\n",
      "loss: 1030.994746334076\n",
      "loss: 1043.7558387699128\n",
      "loss: 1024.8857666931153\n",
      "Avg loss: 1086.619873046875!\n",
      "Epoch 214-----------------------------\n",
      "loss: 1.346032958984375\n",
      "loss: 1029.9527781810762\n",
      "loss: 1043.8936056079865\n",
      "loss: 1024.2936045360566\n",
      "Avg loss: 1085.8017578125!\n",
      "Epoch 215-----------------------------\n",
      "loss: 1.3373564453125\n",
      "loss: 1029.3542612609863\n",
      "loss: 1043.4317044334412\n",
      "loss: 1023.1631288776398\n",
      "Avg loss: 1085.1590576171875!\n",
      "Epoch 216-----------------------------\n",
      "loss: 1.330076416015625\n",
      "loss: 1028.8707353572845\n",
      "loss: 1042.465839586258\n",
      "loss: 1021.9359472122193\n",
      "Avg loss: 1084.2236328125!\n",
      "Epoch 217-----------------------------\n",
      "loss: 1.3238671875\n",
      "loss: 1027.9703445358277\n",
      "loss: 1042.3924728603363\n",
      "loss: 1021.6043252201081\n",
      "Avg loss: 1083.94677734375!\n",
      "Epoch 218-----------------------------\n",
      "loss: 1.3170068359375\n",
      "loss: 1027.8137893505098\n",
      "loss: 1041.7364153289795\n",
      "loss: 1020.4218850688934\n",
      "Avg loss: 1082.952392578125!\n",
      "Epoch 219-----------------------------\n",
      "loss: 1.3100562744140625\n",
      "loss: 1026.8178415985108\n",
      "loss: 1041.5776784629822\n",
      "loss: 1019.9844213027955\n",
      "Avg loss: 1082.389892578125!\n",
      "Epoch 220-----------------------------\n",
      "loss: 1.304902099609375\n",
      "loss: 1026.3566234054565\n",
      "loss: 1040.7906034469604\n",
      "loss: 1018.9386826152802\n",
      "Avg loss: 1081.7337646484375!\n",
      "Epoch 221-----------------------------\n",
      "loss: 1.2986727294921876\n",
      "loss: 1025.7608971652985\n",
      "loss: 1040.6738806095123\n",
      "loss: 1018.6109056549072\n",
      "Avg loss: 1081.3878173828125!\n",
      "Epoch 222-----------------------------\n",
      "loss: 1.2917347412109375\n",
      "loss: 1025.5332227954864\n",
      "loss: 1039.882545589447\n",
      "loss: 1017.4163286762238\n",
      "Avg loss: 1080.5235595703125!\n",
      "Epoch 223-----------------------------\n",
      "loss: 1.2831304931640626\n",
      "loss: 1024.7420886173247\n",
      "loss: 1039.7295107135774\n",
      "loss: 1017.0299391765594\n",
      "Avg loss: 1080.2135009765625!\n",
      "Epoch 224-----------------------------\n",
      "loss: 1.2762283935546874\n",
      "loss: 1024.455913105011\n",
      "loss: 1038.8627641296387\n",
      "loss: 1015.8891002063751\n",
      "Avg loss: 1079.234619140625!\n",
      "Epoch 225-----------------------------\n",
      "loss: 1.269657470703125\n",
      "loss: 1023.6328989353179\n",
      "loss: 1038.8179992961884\n",
      "loss: 1015.5293561229706\n",
      "Avg loss: 1079.0009765625!\n",
      "Epoch 226-----------------------------\n",
      "loss: 1.2615965576171875\n",
      "loss: 1023.5194724330902\n",
      "loss: 1038.7454868927002\n",
      "loss: 1015.2213202152252\n",
      "Avg loss: 1078.80224609375!\n",
      "Epoch 227-----------------------------\n",
      "loss: 1.2558072509765625\n",
      "loss: 1023.3523782711029\n",
      "loss: 1038.643704946518\n",
      "loss: 1014.7213544712066\n",
      "Avg loss: 1078.29296875!\n",
      "Epoch 228-----------------------------\n",
      "loss: 1.2496168212890626\n",
      "loss: 1022.9923099079132\n",
      "loss: 1037.4916285266877\n",
      "loss: 1013.3857380027771\n",
      "Avg loss: 1077.05859375!\n",
      "Epoch 229-----------------------------\n",
      "loss: 1.2452454833984374\n",
      "loss: 1021.9545186233521\n",
      "loss: 1037.6069861869812\n",
      "loss: 1013.1200078220368\n",
      "Avg loss: 1076.61181640625!\n",
      "Epoch 230-----------------------------\n",
      "loss: 1.2397615966796875\n",
      "loss: 1021.6140635204315\n",
      "loss: 1037.4151048069\n",
      "loss: 1011.9702315139771\n",
      "Avg loss: 1076.01025390625!\n",
      "Epoch 231-----------------------------\n",
      "loss: 1.233105712890625\n",
      "loss: 1021.0625612812042\n",
      "loss: 1036.5401042289734\n",
      "loss: 1010.9290344257355\n",
      "Avg loss: 1074.913330078125!\n",
      "Epoch 232-----------------------------\n",
      "loss: 1.225964599609375\n",
      "loss: 1020.2079214019775\n",
      "loss: 1036.4418291435243\n",
      "loss: 1010.6309954032898\n",
      "Avg loss: 1074.5997314453125!\n",
      "Epoch 233-----------------------------\n",
      "loss: 1.2227564697265625\n",
      "loss: 1020.1004818153382\n",
      "loss: 1036.2436773128509\n",
      "loss: 1010.1135956325531\n",
      "Avg loss: 1074.3529052734375!\n",
      "Epoch 234-----------------------------\n",
      "loss: 1.2168236083984374\n",
      "loss: 1019.8458072013855\n",
      "loss: 1035.9622553024292\n",
      "loss: 1009.6889743862152\n",
      "Avg loss: 1073.8798828125!\n",
      "Epoch 235-----------------------------\n",
      "loss: 1.212327392578125\n",
      "loss: 1019.5631881847381\n",
      "loss: 1035.221299659729\n",
      "loss: 1008.6744948501587\n",
      "Avg loss: 1072.9813232421875!\n",
      "Epoch 236-----------------------------\n",
      "loss: 1.2074866943359375\n",
      "loss: 1018.7598567714691\n",
      "loss: 1035.1761431884765\n",
      "loss: 1008.3640220222474\n",
      "Avg loss: 1072.7857666015625!\n",
      "Epoch 237-----------------------------\n",
      "loss: 1.2014847412109375\n",
      "loss: 1018.596907491684\n",
      "loss: 1034.9540715808869\n",
      "loss: 1007.9152564983368\n",
      "Avg loss: 1072.1956787109375!\n",
      "Epoch 238-----------------------------\n",
      "loss: 1.1984385986328125\n",
      "loss: 1018.2278224086762\n",
      "loss: 1034.6095480937959\n",
      "loss: 1007.3143255004883\n",
      "Avg loss: 1071.854736328125!\n",
      "Epoch 239-----------------------------\n",
      "loss: 1.193588134765625\n",
      "loss: 1017.9543023014069\n",
      "loss: 1033.7696635627747\n",
      "loss: 1006.3069145965576\n",
      "Avg loss: 1070.87158203125!\n",
      "Epoch 240-----------------------------\n",
      "loss: 1.18892431640625\n",
      "loss: 1017.127607290268\n",
      "loss: 1033.767275949478\n",
      "loss: 1005.9454566745758\n",
      "Avg loss: 1070.1915283203125!\n",
      "Epoch 241-----------------------------\n",
      "loss: 1.1848629150390626\n",
      "loss: 1016.588928440094\n",
      "loss: 1033.4634529571533\n",
      "loss: 1004.660909860611\n",
      "Avg loss: 1069.3536376953125!\n",
      "Epoch 242-----------------------------\n",
      "loss: 1.1801580810546874\n",
      "loss: 1015.9918515701294\n",
      "loss: 1032.8046009368898\n",
      "loss: 1004.1319523525239\n",
      "Avg loss: 1068.9154052734375!\n",
      "Epoch 243-----------------------------\n",
      "loss: 1.1763677978515625\n",
      "loss: 1015.7075935020447\n",
      "loss: 1032.5392343711853\n",
      "loss: 1003.627510597229\n",
      "Avg loss: 1068.4080810546875!\n",
      "Epoch 244-----------------------------\n",
      "loss: 1.1722059326171874\n",
      "loss: 1015.2847193984985\n",
      "loss: 1032.2965143375397\n",
      "loss: 1003.1192706356048\n",
      "Avg loss: 1067.8780517578125!\n",
      "Epoch 245-----------------------------\n",
      "loss: 1.168149658203125\n",
      "loss: 1014.7708718929291\n",
      "loss: 1031.2856004562377\n",
      "loss: 1002.2220237751008\n",
      "Avg loss: 1067.073486328125!\n",
      "Epoch 246-----------------------------\n",
      "loss: 1.16360888671875\n",
      "loss: 1014.1552183570861\n",
      "loss: 1031.195565782547\n",
      "loss: 1001.9838791255951\n",
      "Avg loss: 1066.836181640625!\n",
      "Epoch 247-----------------------------\n",
      "loss: 1.1602413330078125\n",
      "loss: 1013.9782593803405\n",
      "loss: 1031.0834434185028\n",
      "loss: 1001.4926162948608\n",
      "Avg loss: 1066.13818359375!\n",
      "Epoch 248-----------------------------\n",
      "loss: 1.1563697509765625\n",
      "loss: 1013.3543699378968\n",
      "loss: 1030.8409040164947\n",
      "loss: 1000.9325462379455\n",
      "Avg loss: 1066.4779052734375!\n",
      "Epoch 249-----------------------------\n",
      "loss: 1.154302734375\n",
      "loss: 1013.524706407547\n",
      "loss: 1031.1575011425018\n",
      "loss: 1000.5485665016174\n",
      "Avg loss: 1065.9140625!\n",
      "Epoch 250-----------------------------\n",
      "loss: 1.150057861328125\n",
      "loss: 1013.00903972435\n",
      "loss: 1030.255760482788\n",
      "loss: 999.9463647975922\n",
      "Avg loss: 1065.5858154296875!\n",
      "Epoch 251-----------------------------\n",
      "loss: 1.148732666015625\n",
      "loss: 1012.5578028812408\n",
      "loss: 1030.4315645656586\n",
      "loss: 999.5129304561615\n",
      "Avg loss: 1065.2828369140625!\n",
      "Epoch 252-----------------------------\n",
      "loss: 1.1444476318359376\n",
      "loss: 1012.3811717700959\n",
      "loss: 1030.261465335846\n",
      "loss: 999.1465580558777\n",
      "Avg loss: 1064.82275390625!\n",
      "Epoch 253-----------------------------\n",
      "loss: 1.1416959228515624\n",
      "loss: 1011.9971012783051\n",
      "loss: 1029.9403982028962\n",
      "loss: 998.6693371925354\n",
      "Avg loss: 1064.5374755859375!\n",
      "Epoch 254-----------------------------\n",
      "loss: 1.13777734375\n",
      "loss: 1011.7383003520965\n",
      "loss: 1029.816633687973\n",
      "loss: 998.6639666557312\n",
      "Avg loss: 1064.73828125!\n",
      "Epoch 255-----------------------------\n",
      "loss: 1.136692626953125\n",
      "loss: 1011.7973495464325\n",
      "loss: 1029.7926893367767\n",
      "loss: 997.255161951065\n",
      "Avg loss: 1063.5941162109375!\n",
      "Epoch 256-----------------------------\n",
      "loss: 1.129890625\n",
      "loss: 1010.8220288658142\n",
      "loss: 1029.4885518798828\n",
      "loss: 996.8164570255279\n",
      "Avg loss: 1063.170166015625!\n",
      "Epoch 257-----------------------------\n",
      "loss: 1.126869140625\n",
      "loss: 1010.3610323581696\n",
      "loss: 1029.0378662509918\n",
      "loss: 996.8486924514771\n",
      "Avg loss: 1063.7364501953125!\n",
      "Epoch 258-----------------------------\n",
      "loss: 1.1253280029296875\n",
      "loss: 1010.7997059364319\n",
      "loss: 1028.8649348983765\n",
      "loss: 995.9621105213165\n",
      "Avg loss: 1062.6168212890625!\n",
      "Epoch 259-----------------------------\n",
      "loss: 1.1203839111328124\n",
      "loss: 1009.8609607543946\n",
      "loss: 1028.5455975151062\n",
      "loss: 995.517574886322\n",
      "Avg loss: 1062.368408203125!\n",
      "Epoch 260-----------------------------\n",
      "loss: 1.1170238037109375\n",
      "loss: 1009.6656745758056\n",
      "loss: 1028.4333249168396\n",
      "loss: 995.1898016757965\n",
      "Avg loss: 1062.0673828125!\n",
      "Epoch 261-----------------------------\n",
      "loss: 1.1156978759765626\n",
      "loss: 1009.32545419693\n",
      "loss: 1028.0917560462951\n",
      "loss: 995.0587990512848\n",
      "Avg loss: 1062.1185302734375!\n",
      "Epoch 262-----------------------------\n",
      "loss: 1.115555908203125\n",
      "loss: 1009.2845607528686\n",
      "loss: 1028.2257653007507\n",
      "loss: 993.8861902313232\n",
      "Avg loss: 1061.1231689453125!\n",
      "Epoch 263-----------------------------\n",
      "loss: 1.1119652099609374\n",
      "loss: 1008.4518418540955\n",
      "loss: 1027.4840452728272\n",
      "loss: 993.6378839988708\n",
      "Avg loss: 1061.420654296875!\n",
      "Epoch 264-----------------------------\n",
      "loss: 1.1108455810546876\n",
      "loss: 1008.5399106941223\n",
      "loss: 1027.1050074005127\n",
      "loss: 992.6876830692291\n",
      "Avg loss: 1060.1981201171875!\n",
      "Epoch 265-----------------------------\n",
      "loss: 1.1065361328125\n",
      "loss: 1007.6035914077759\n",
      "loss: 1026.7710410614013\n",
      "loss: 992.2485314159393\n",
      "Avg loss: 1060.065185546875!\n",
      "Epoch 266-----------------------------\n",
      "loss: 1.1049564208984375\n",
      "loss: 1007.2962112770081\n",
      "loss: 1026.5035420093536\n",
      "loss: 992.1780830783844\n",
      "Avg loss: 1060.0673828125!\n",
      "Epoch 267-----------------------------\n",
      "loss: 1.105429931640625\n",
      "loss: 1007.2298920078277\n",
      "loss: 1026.3887849597932\n",
      "loss: 991.5395135555267\n",
      "Avg loss: 1059.625!\n",
      "Epoch 268-----------------------------\n",
      "loss: 1.101343505859375\n",
      "loss: 1006.8465020484924\n",
      "loss: 1025.9761546497346\n",
      "loss: 991.3050451526642\n",
      "Avg loss: 1059.302001953125!\n",
      "Epoch 269-----------------------------\n",
      "loss: 1.10187548828125\n",
      "loss: 1006.5016473369599\n",
      "loss: 1025.9368070869446\n",
      "loss: 990.1261229114532\n",
      "Avg loss: 1058.488037109375!\n",
      "Epoch 270-----------------------------\n",
      "loss: 1.0981796875\n",
      "loss: 1005.9120052680969\n",
      "loss: 1025.4632457199098\n",
      "loss: 989.5717336292266\n",
      "Avg loss: 1058.0205078125!\n",
      "Epoch 271-----------------------------\n",
      "loss: 1.096076171875\n",
      "loss: 1005.2615448093414\n",
      "loss: 1024.45348755455\n",
      "loss: 988.9254482707977\n",
      "Avg loss: 1057.5716552734375!\n",
      "Epoch 272-----------------------------\n",
      "loss: 1.09473095703125\n",
      "loss: 1004.7971582775116\n",
      "loss: 1024.4785398426056\n",
      "loss: 988.4785606822968\n",
      "Avg loss: 1057.3128662109375!\n",
      "Epoch 273-----------------------------\n",
      "loss: 1.091625732421875\n",
      "loss: 1004.6037039585113\n",
      "loss: 1024.2198904953002\n",
      "loss: 988.4286812858581\n",
      "Avg loss: 1057.3079833984375!\n",
      "Epoch 274-----------------------------\n",
      "loss: 1.0919215087890626\n",
      "loss: 1004.5690972824096\n",
      "loss: 1024.1207534809112\n",
      "loss: 987.7243746623993\n",
      "Avg loss: 1056.57763671875!\n",
      "Epoch 275-----------------------------\n",
      "loss: 1.09019580078125\n",
      "loss: 1003.9423927707672\n",
      "loss: 1023.998332611084\n",
      "loss: 987.2514358673096\n",
      "Avg loss: 1056.4306640625!\n",
      "Epoch 276-----------------------------\n",
      "loss: 1.090321044921875\n",
      "loss: 1003.6809505386352\n",
      "loss: 1023.770896982193\n",
      "loss: 986.6506440334321\n",
      "Avg loss: 1056.0067138671875!\n",
      "Epoch 277-----------------------------\n",
      "loss: 1.0865855712890624\n",
      "loss: 1003.3409086303711\n",
      "loss: 1022.8945182952881\n",
      "loss: 986.0372425022125\n",
      "Avg loss: 1055.4888916015625!\n",
      "Epoch 278-----------------------------\n",
      "loss: 1.085052490234375\n",
      "loss: 1002.7042327861786\n",
      "loss: 1022.9346660842896\n",
      "loss: 985.5674729099273\n",
      "Avg loss: 1055.0283203125!\n",
      "Epoch 279-----------------------------\n",
      "loss: 1.080767822265625\n",
      "loss: 1002.3402732372284\n",
      "loss: 1022.3109980754853\n",
      "loss: 985.3599461727142\n",
      "Avg loss: 1055.084228515625!\n",
      "Epoch 280-----------------------------\n",
      "loss: 1.08097509765625\n",
      "loss: 1002.3524376125335\n",
      "loss: 1022.4325236186982\n",
      "loss: 984.766068977356\n",
      "Avg loss: 1054.225830078125!\n",
      "Epoch 281-----------------------------\n",
      "loss: 1.0786005859375\n",
      "loss: 1001.67499111557\n",
      "loss: 1021.9559370861053\n",
      "loss: 984.0781277961731\n",
      "Avg loss: 1054.09765625!\n",
      "Epoch 282-----------------------------\n",
      "loss: 1.077486328125\n",
      "loss: 1001.3866061077118\n",
      "loss: 1021.9177365932464\n",
      "loss: 983.4755282096863\n",
      "Avg loss: 1053.3743896484375!\n",
      "Epoch 283-----------------------------\n",
      "loss: 1.0730020751953124\n",
      "loss: 1000.6807609844208\n",
      "loss: 1021.1524209270477\n",
      "loss: 983.083209733963\n",
      "Avg loss: 1053.349853515625!\n",
      "Epoch 284-----------------------------\n",
      "loss: 1.0725394287109375\n",
      "loss: 1000.5398147907257\n",
      "loss: 1021.1002694854736\n",
      "loss: 982.6008490085602\n",
      "Avg loss: 1052.8441162109375!\n",
      "Epoch 285-----------------------------\n",
      "loss: 1.068106201171875\n",
      "loss: 1000.1181246490479\n",
      "loss: 1020.166918264389\n",
      "loss: 981.9227996711732\n",
      "Avg loss: 1052.4825439453125!\n",
      "Epoch 286-----------------------------\n",
      "loss: 1.066416748046875\n",
      "loss: 999.6650283222199\n",
      "loss: 1020.3160382556915\n",
      "loss: 981.4793952407837\n",
      "Avg loss: 1051.7337646484375!\n",
      "Epoch 287-----------------------------\n",
      "loss: 1.062636474609375\n",
      "loss: 999.0849843578338\n",
      "loss: 1020.0023775482177\n",
      "loss: 981.03246900177\n",
      "Avg loss: 1051.8818359375!\n",
      "Epoch 288-----------------------------\n",
      "loss: 1.06385791015625\n",
      "loss: 999.1366271476745\n",
      "loss: 1019.9596541004181\n",
      "loss: 980.4788373203278\n",
      "Avg loss: 1051.2642822265625!\n",
      "Epoch 289-----------------------------\n",
      "loss: 1.0604534912109376\n",
      "loss: 998.4880030612945\n",
      "loss: 1019.4225938682556\n",
      "loss: 980.2701404457092\n",
      "Avg loss: 1051.307373046875!\n",
      "Epoch 290-----------------------------\n",
      "loss: 1.060163330078125\n",
      "loss: 998.4774073200226\n",
      "loss: 1019.2814025745391\n",
      "loss: 979.7223889160156\n",
      "Avg loss: 1050.86083984375!\n",
      "Epoch 291-----------------------------\n",
      "loss: 1.055548583984375\n",
      "loss: 998.1066225776673\n",
      "loss: 1018.8331188602448\n",
      "loss: 979.4909326171875\n",
      "Avg loss: 1050.6099853515625!\n",
      "Epoch 292-----------------------------\n",
      "loss: 1.0554642333984374\n",
      "loss: 997.8675229091644\n",
      "loss: 1018.7282538204194\n",
      "loss: 978.4243353176117\n",
      "Avg loss: 1049.813720703125!\n",
      "Epoch 293-----------------------------\n",
      "loss: 1.0502933349609376\n",
      "loss: 997.0362189025878\n",
      "loss: 1018.0580278644562\n",
      "loss: 978.2614021816254\n",
      "Avg loss: 1049.96337890625!\n",
      "Epoch 294-----------------------------\n",
      "loss: 1.049428955078125\n",
      "loss: 997.1672426395417\n",
      "loss: 1017.6812905197144\n",
      "loss: 977.408181798935\n",
      "Avg loss: 1049.0377197265625!\n",
      "Epoch 295-----------------------------\n",
      "loss: 1.043538330078125\n",
      "loss: 996.2316692790985\n",
      "loss: 1017.288850484848\n",
      "loss: 977.3020223312378\n",
      "Avg loss: 1049.18212890625!\n",
      "Epoch 296-----------------------------\n",
      "loss: 1.0435208740234374\n",
      "loss: 996.3847490921021\n",
      "loss: 1017.2702397785187\n",
      "loss: 976.824108882904\n",
      "Avg loss: 1048.6583251953125!\n",
      "Epoch 297-----------------------------\n",
      "loss: 1.0407890625\n",
      "loss: 995.8648902225494\n",
      "loss: 1017.0605687026978\n",
      "loss: 976.2950395755768\n",
      "Avg loss: 1048.498779296875!\n",
      "Epoch 298-----------------------------\n",
      "loss: 1.0391947021484376\n",
      "loss: 995.703554107666\n",
      "loss: 1016.8401156482696\n",
      "loss: 975.7578267765045\n",
      "Avg loss: 1048.0093994140625!\n",
      "Epoch 299-----------------------------\n",
      "loss: 1.0354910888671875\n",
      "loss: 995.1577272720336\n",
      "loss: 1016.3918103885651\n",
      "loss: 975.7194432125092\n",
      "Avg loss: 1048.23486328125!\n",
      "Epoch 300-----------------------------\n",
      "loss: 1.0357855224609376\n",
      "loss: 995.3983589019775\n",
      "loss: 1016.5268014297485\n",
      "loss: 975.3480448360443\n",
      "Avg loss: 1048.00830078125!\n",
      "Epoch 301-----------------------------\n",
      "loss: 1.032322021484375\n",
      "loss: 995.1201493377686\n",
      "loss: 1015.8696974334716\n",
      "loss: 974.7976568565368\n",
      "Avg loss: 1047.412109375!\n",
      "Epoch 302-----------------------------\n",
      "loss: 1.0308330078125\n",
      "loss: 994.5355194549561\n",
      "loss: 1016.0344846801758\n",
      "loss: 974.1594826183319\n",
      "Avg loss: 1047.0350341796875!\n",
      "Epoch 303-----------------------------\n",
      "loss: 1.026610107421875\n",
      "loss: 994.1305453166962\n",
      "loss: 1015.636552570343\n",
      "loss: 974.0549033145904\n",
      "Avg loss: 1047.1884765625!\n",
      "Epoch 304-----------------------------\n",
      "loss: 1.0272401123046875\n",
      "loss: 994.2451361770629\n",
      "loss: 1015.879586139679\n",
      "loss: 973.9074160270691\n",
      "Avg loss: 1047.1312255859375!\n",
      "Epoch 305-----------------------------\n",
      "loss: 1.0268878173828124\n",
      "loss: 994.0947458820343\n",
      "loss: 1015.7222566108703\n",
      "loss: 973.4941429290772\n",
      "Avg loss: 1046.7923583984375!\n",
      "Epoch 306-----------------------------\n",
      "loss: 1.0228372802734376\n",
      "loss: 993.6963121089935\n",
      "loss: 1015.4415786781311\n",
      "loss: 973.2520730533599\n",
      "Avg loss: 1046.6278076171875!\n",
      "Epoch 307-----------------------------\n",
      "loss: 1.02289306640625\n",
      "loss: 993.4467631587983\n",
      "loss: 1015.3148146686553\n",
      "loss: 972.4469180030823\n",
      "Avg loss: 1046.087158203125!\n",
      "Epoch 308-----------------------------\n",
      "loss: 1.0179354248046875\n",
      "loss: 992.8356399822235\n",
      "loss: 1014.946701587677\n",
      "loss: 972.3319858932496\n",
      "Avg loss: 1046.3121337890625!\n",
      "Epoch 309-----------------------------\n",
      "loss: 1.0171355590820312\n",
      "loss: 993.0677484531402\n",
      "loss: 1014.6925162124634\n",
      "loss: 971.8797922344207\n",
      "Avg loss: 1045.9266357421875!\n",
      "Epoch 310-----------------------------\n",
      "loss: 1.0153584594726563\n",
      "loss: 992.4937321758271\n",
      "loss: 1014.6923969707489\n",
      "loss: 971.6751762237549\n",
      "Avg loss: 1045.8033447265625!\n",
      "Epoch 311-----------------------------\n",
      "loss: 1.013764892578125\n",
      "loss: 992.584468542099\n",
      "loss: 1014.670628326416\n",
      "loss: 971.623726070404\n",
      "Avg loss: 1045.9822998046875!\n",
      "Epoch 312-----------------------------\n",
      "loss: 1.0159024047851561\n",
      "loss: 992.5054739837647\n",
      "loss: 1014.5473989086152\n",
      "loss: 971.5172487449646\n",
      "Avg loss: 1045.834716796875!\n",
      "Epoch 313-----------------------------\n",
      "loss: 1.017301025390625\n",
      "loss: 992.358657705307\n",
      "loss: 1014.732226278305\n",
      "loss: 970.7051856765747\n",
      "Avg loss: 1045.3031005859375!\n",
      "Epoch 314-----------------------------\n",
      "loss: 1.0113707275390624\n",
      "loss: 991.7834095973968\n",
      "loss: 1014.1026536579132\n",
      "loss: 970.6088085041046\n",
      "Avg loss: 1045.3751220703125!\n",
      "Epoch 315-----------------------------\n",
      "loss: 1.0115045166015626\n",
      "loss: 991.8972907180786\n",
      "loss: 1014.1406128025055\n",
      "loss: 970.5858849372863\n",
      "Avg loss: 1045.55224609375!\n",
      "Epoch 316-----------------------------\n",
      "loss: 1.0139418334960937\n",
      "loss: 991.904215297699\n",
      "loss: 1013.9930250720978\n",
      "loss: 969.8766754837036\n",
      "Avg loss: 1044.7684326171875!\n",
      "Epoch 317-----------------------------\n",
      "loss: 1.0076483154296876\n",
      "loss: 991.2711885948181\n",
      "loss: 1013.6454522380828\n",
      "loss: 969.9465905170441\n",
      "Avg loss: 1045.11865234375!\n",
      "Epoch 318-----------------------------\n",
      "loss: 1.0107774047851563\n",
      "loss: 991.4694016113282\n",
      "loss: 1013.7646041927337\n",
      "loss: 969.7929417934417\n",
      "Avg loss: 1044.9993896484375!\n",
      "Epoch 319-----------------------------\n",
      "loss: 1.010743408203125\n",
      "loss: 991.2846339588166\n",
      "loss: 1013.6842895965576\n",
      "loss: 969.4184135093689\n",
      "Avg loss: 1044.544921875!\n",
      "Epoch 320-----------------------------\n",
      "loss: 1.0058680419921875\n",
      "loss: 990.907652206421\n",
      "loss: 1013.3169636821747\n",
      "loss: 968.8392438869477\n",
      "Avg loss: 1044.30615234375!\n",
      "Epoch 321-----------------------------\n",
      "loss: 1.0035508422851562\n",
      "loss: 990.6986311779023\n",
      "loss: 1013.2230954055786\n",
      "loss: 968.6776220741272\n",
      "Avg loss: 1044.2542724609375!\n",
      "Epoch 322-----------------------------\n",
      "loss: 1.0048067016601563\n",
      "loss: 990.6319939155578\n",
      "loss: 1013.3148782024383\n",
      "loss: 968.3117982692719\n",
      "Avg loss: 1043.713134765625!\n",
      "Epoch 323-----------------------------\n",
      "loss: 0.9994699096679688\n",
      "loss: 990.0689515514374\n",
      "loss: 1012.608889453888\n",
      "loss: 968.0476817932129\n",
      "Avg loss: 1043.8724365234375!\n",
      "Epoch 324-----------------------------\n",
      "loss: 0.9988836669921874\n",
      "loss: 990.2034891014099\n",
      "loss: 1012.6038383560181\n",
      "loss: 967.8747082767486\n",
      "Avg loss: 1043.70849609375!\n",
      "Epoch 325-----------------------------\n",
      "loss: 0.9998734741210937\n",
      "loss: 990.0712896270752\n",
      "loss: 1012.6544944610596\n",
      "loss: 967.771854560852\n",
      "Avg loss: 1043.72119140625!\n",
      "Epoch 326-----------------------------\n",
      "loss: 1.0012914428710937\n",
      "loss: 989.8979538040161\n",
      "loss: 1012.4072738418579\n",
      "loss: 967.235788860321\n",
      "Avg loss: 1043.06982421875!\n",
      "Epoch 327-----------------------------\n",
      "loss: 0.9967885131835937\n",
      "loss: 989.2827118263244\n",
      "loss: 1012.0358441123963\n",
      "loss: 966.7479560756683\n",
      "Avg loss: 1042.9609375!\n",
      "Epoch 328-----------------------------\n",
      "loss: 0.994904052734375\n",
      "loss: 989.2307953624726\n",
      "loss: 1012.0123578739166\n",
      "loss: 966.6577027320861\n",
      "Avg loss: 1042.9755859375!\n",
      "Epoch 329-----------------------------\n",
      "loss: 0.9944732666015625\n",
      "loss: 989.3495419197083\n",
      "loss: 1012.1893289661407\n",
      "loss: 966.1809497070312\n",
      "Avg loss: 1042.3751220703125!\n",
      "Epoch 330-----------------------------\n",
      "loss: 0.9902745361328125\n",
      "loss: 988.56250989151\n",
      "loss: 1011.4259728336334\n",
      "loss: 966.0391015739441\n",
      "Avg loss: 1042.57470703125!\n",
      "Epoch 331-----------------------------\n",
      "loss: 0.9907554321289063\n",
      "loss: 988.8843555927276\n",
      "loss: 1011.6113038959503\n",
      "loss: 965.7975735778808\n",
      "Avg loss: 1042.321044921875!\n",
      "Epoch 332-----------------------------\n",
      "loss: 0.9879256591796876\n",
      "loss: 988.482484582901\n",
      "loss: 1011.1679972629547\n",
      "loss: 965.661341014862\n",
      "Avg loss: 1042.4384765625!\n",
      "Epoch 333-----------------------------\n",
      "loss: 0.9911740112304688\n",
      "loss: 988.7173453235627\n",
      "loss: 1011.5225582389832\n",
      "loss: 965.4550764217377\n",
      "Avg loss: 1042.177978515625!\n",
      "Epoch 334-----------------------------\n",
      "loss: 0.98889990234375\n",
      "loss: 988.303260591507\n",
      "loss: 1011.1344879932403\n",
      "loss: 964.7982694835663\n",
      "Avg loss: 1041.7733154296875!\n",
      "Epoch 335-----------------------------\n",
      "loss: 0.9866885986328126\n",
      "loss: 987.9267005748749\n",
      "loss: 1011.0300620651245\n",
      "loss: 964.7395031681061\n",
      "Avg loss: 1041.8756103515625!\n",
      "Epoch 336-----------------------------\n",
      "loss: 0.985555908203125\n",
      "loss: 988.0718996047974\n",
      "loss: 1011.00058584404\n",
      "loss: 964.5017770805359\n",
      "Avg loss: 1041.5224609375!\n",
      "Epoch 337-----------------------------\n",
      "loss: 0.984384033203125\n",
      "loss: 987.8272109928131\n",
      "loss: 1010.9324464244843\n",
      "loss: 964.3199142074585\n",
      "Avg loss: 1041.4556884765625!\n",
      "Epoch 338-----------------------------\n",
      "loss: 0.9813270874023438\n",
      "loss: 987.7227781829833\n",
      "loss: 1010.711779876709\n",
      "loss: 964.0235164146424\n",
      "Avg loss: 1041.2706298828125!\n",
      "Epoch 339-----------------------------\n",
      "loss: 0.9796527099609375\n",
      "loss: 987.4355068626404\n",
      "loss: 1010.3110072994232\n",
      "loss: 964.0486995620728\n",
      "Avg loss: 1041.497314453125!\n",
      "Epoch 340-----------------------------\n",
      "loss: 0.9819818115234376\n",
      "loss: 987.6661907958984\n",
      "loss: 1010.6628263320923\n",
      "loss: 963.6096434020997\n",
      "Avg loss: 1040.5562744140625!\n",
      "Epoch 341-----------------------------\n",
      "loss: 0.9773008422851562\n",
      "loss: 986.8255922298431\n",
      "loss: 1010.0465807189942\n",
      "loss: 963.126258102417\n",
      "Avg loss: 1040.6741943359375!\n",
      "Epoch 342-----------------------------\n",
      "loss: 0.9769982299804687\n",
      "loss: 986.8267598304749\n",
      "loss: 1010.2864524116516\n",
      "loss: 963.0988719520569\n",
      "Avg loss: 1040.8082275390625!\n",
      "Epoch 343-----------------------------\n",
      "loss: 0.9774774780273437\n",
      "loss: 987.1052242584228\n",
      "loss: 1010.4374629001618\n",
      "loss: 962.9150297164917\n",
      "Avg loss: 1040.529296875!\n",
      "Epoch 344-----------------------------\n",
      "loss: 0.9734890747070313\n",
      "loss: 986.6572917690277\n",
      "loss: 1010.0306294574738\n",
      "loss: 962.7992835597992\n",
      "Avg loss: 1040.5726318359375!\n",
      "Epoch 345-----------------------------\n",
      "loss: 0.9754500122070312\n",
      "loss: 986.8921163578034\n",
      "loss: 1010.2505445842743\n",
      "loss: 962.5274509181976\n",
      "Avg loss: 1040.0140380859375!\n",
      "Epoch 346-----------------------------\n",
      "loss: 0.9678013916015625\n",
      "loss: 986.2478903255462\n",
      "loss: 1009.783470375061\n",
      "loss: 962.4139746952056\n",
      "Avg loss: 1040.2176513671875!\n",
      "Epoch 347-----------------------------\n",
      "loss: 0.9697427368164062\n",
      "loss: 986.4866093215942\n",
      "loss: 1009.8129060020447\n",
      "loss: 962.3221859550476\n",
      "Avg loss: 1040.21826171875!\n",
      "Epoch 348-----------------------------\n",
      "loss: 0.9701483154296875\n",
      "loss: 986.5946160964966\n",
      "loss: 1010.0404259300232\n",
      "loss: 961.8617849121093\n",
      "Avg loss: 1039.15576171875!\n",
      "Epoch 349-----------------------------\n",
      "loss: 0.96104541015625\n",
      "loss: 985.4367777976989\n",
      "loss: 1009.0534688549042\n",
      "loss: 961.3275014400482\n",
      "Avg loss: 1039.511962890625!\n",
      "Epoch 350-----------------------------\n",
      "loss: 0.964822021484375\n",
      "loss: 985.9108511962891\n",
      "loss: 1009.714573802948\n",
      "loss: 961.4043576183319\n",
      "Avg loss: 1039.4869384765625!\n",
      "Epoch 351-----------------------------\n",
      "loss: 0.9640496215820312\n",
      "loss: 985.7698772735596\n",
      "loss: 1009.5206978874206\n",
      "loss: 961.0575614395142\n",
      "Avg loss: 1039.0101318359375!\n",
      "Epoch 352-----------------------------\n",
      "loss: 0.9570119018554688\n",
      "loss: 985.5030156822205\n",
      "loss: 1009.0649179058075\n",
      "loss: 960.9631536483764\n",
      "Avg loss: 1039.128662109375!\n",
      "Epoch 353-----------------------------\n",
      "loss: 0.958038818359375\n",
      "loss: 985.5295067615509\n",
      "loss: 1009.179024427414\n",
      "loss: 960.6935322151184\n",
      "Avg loss: 1038.5999755859375!\n",
      "Epoch 354-----------------------------\n",
      "loss: 0.9543749389648437\n",
      "loss: 985.1024537849427\n",
      "loss: 1008.8784876003266\n",
      "loss: 960.8932222042084\n",
      "Avg loss: 1039.2159423828125!\n",
      "Epoch 355-----------------------------\n",
      "loss: 0.9589947509765625\n",
      "loss: 985.6450819396973\n",
      "loss: 1009.2892254714966\n",
      "loss: 960.6587465744019\n",
      "Avg loss: 1038.8397216796875!\n",
      "Epoch 356-----------------------------\n",
      "loss: 0.9552398071289062\n",
      "loss: 985.3093502120971\n",
      "loss: 1009.0404525165558\n",
      "loss: 960.3735659999847\n",
      "Avg loss: 1038.264404296875!\n",
      "Epoch 357-----------------------------\n",
      "loss: 0.9512916259765625\n",
      "loss: 984.69635039711\n",
      "loss: 1008.6024960918427\n",
      "loss: 959.750884519577\n",
      "Avg loss: 1038.24560546875!\n",
      "Epoch 358-----------------------------\n",
      "loss: 0.9501990356445312\n",
      "loss: 984.7414718055725\n",
      "loss: 1008.6794239368438\n",
      "loss: 959.6512516536712\n",
      "Avg loss: 1038.0753173828125!\n",
      "Epoch 359-----------------------------\n",
      "loss: 0.9469395751953125\n",
      "loss: 984.562594039917\n",
      "loss: 1008.4151994037628\n",
      "loss: 959.6093778514862\n",
      "Avg loss: 1038.1044921875!\n",
      "Epoch 360-----------------------------\n",
      "loss: 0.9475418090820312\n",
      "loss: 984.7863838844299\n",
      "loss: 1008.7949838066102\n",
      "loss: 959.4624833889008\n",
      "Avg loss: 1037.6993408203125!\n",
      "Epoch 361-----------------------------\n",
      "loss: 0.9420051879882813\n",
      "loss: 984.2567561950683\n",
      "loss: 1008.1757487564087\n",
      "loss: 959.7684442825317\n",
      "Avg loss: 1038.3956298828125!\n",
      "Epoch 362-----------------------------\n",
      "loss: 0.9493917846679687\n",
      "loss: 985.13263904953\n",
      "loss: 1009.0656803150177\n",
      "loss: 959.6304175682068\n",
      "Avg loss: 1037.9871826171875!\n",
      "Epoch 363-----------------------------\n",
      "loss: 0.9468425903320312\n",
      "loss: 984.6416195011138\n",
      "loss: 1008.7407526264191\n",
      "loss: 959.3959796524048\n",
      "Avg loss: 1037.92529296875!\n",
      "Epoch 364-----------------------------\n",
      "loss: 0.94544970703125\n",
      "loss: 984.6131735095978\n",
      "loss: 1008.4886489067078\n",
      "loss: 959.2768309822083\n",
      "Avg loss: 1037.540771484375!\n",
      "Epoch 365-----------------------------\n",
      "loss: 0.9462880249023438\n",
      "loss: 984.4160765972138\n",
      "loss: 1008.5093159275054\n",
      "loss: 958.6663997821807\n",
      "Avg loss: 1037.315673828125!\n",
      "Epoch 366-----------------------------\n",
      "loss: 0.9431151123046875\n",
      "loss: 983.9685493240356\n",
      "loss: 1008.0959401016236\n",
      "loss: 958.5251342468262\n",
      "Avg loss: 1037.23486328125!\n",
      "Epoch 367-----------------------------\n",
      "loss: 0.9406699829101562\n",
      "loss: 984.0942340431213\n",
      "loss: 1008.077938867569\n",
      "loss: 958.3885515422821\n",
      "Avg loss: 1036.8817138671875!\n",
      "Epoch 368-----------------------------\n",
      "loss: 0.937904052734375\n",
      "loss: 983.8098014354706\n",
      "loss: 1007.9222023983002\n",
      "loss: 958.4563601913452\n",
      "Avg loss: 1037.134521484375!\n",
      "Epoch 369-----------------------------\n",
      "loss: 0.9400731201171875\n",
      "loss: 984.1709350757599\n",
      "loss: 1008.0682981262207\n",
      "loss: 958.344065530777\n",
      "Avg loss: 1037.0050048828125!\n",
      "Epoch 370-----------------------------\n",
      "loss: 0.9384869384765625\n",
      "loss: 983.9215862140655\n",
      "loss: 1007.8379282360077\n",
      "loss: 958.0595228843689\n",
      "Avg loss: 1036.54931640625!\n",
      "Epoch 371-----------------------------\n",
      "loss: 0.9317091674804687\n",
      "loss: 983.6208493213653\n",
      "loss: 1007.5740104103088\n",
      "loss: 958.0956793632507\n",
      "Avg loss: 1036.7786865234375!\n",
      "Epoch 372-----------------------------\n",
      "loss: 0.9341964111328125\n",
      "loss: 984.0594263000488\n",
      "loss: 1007.8647168655395\n",
      "loss: 957.9953879547119\n",
      "Avg loss: 1036.6575927734375!\n",
      "Epoch 373-----------------------------\n",
      "loss: 0.932952392578125\n",
      "loss: 983.8422875995636\n",
      "loss: 1007.6915075664521\n",
      "loss: 957.9578894481659\n",
      "Avg loss: 1036.27392578125!\n",
      "Epoch 374-----------------------------\n",
      "loss: 0.9344932250976562\n",
      "loss: 983.4554230480194\n",
      "loss: 1007.4598510990143\n",
      "loss: 957.3037559242249\n",
      "Avg loss: 1036.1951904296875!\n",
      "Epoch 375-----------------------------\n",
      "loss: 0.9324992065429687\n",
      "loss: 983.4058819580079\n",
      "loss: 1007.507584575653\n",
      "loss: 957.1478868923188\n",
      "Avg loss: 1035.7640380859375!\n",
      "Epoch 376-----------------------------\n",
      "loss: 0.927398193359375\n",
      "loss: 983.180223941803\n",
      "loss: 1007.2816209659576\n",
      "loss: 957.189222158432\n",
      "Avg loss: 1036.0347900390625!\n",
      "Epoch 377-----------------------------\n",
      "loss: 0.930301513671875\n",
      "loss: 983.3870931835174\n",
      "loss: 1007.3620830993652\n",
      "loss: 956.9397230701446\n",
      "Avg loss: 1035.5533447265625!\n",
      "Epoch 378-----------------------------\n",
      "loss: 0.9240933227539062\n",
      "loss: 982.9756565093994\n",
      "loss: 1007.0460049057007\n",
      "loss: 956.8759085197448\n",
      "Avg loss: 1035.628173828125!\n",
      "Epoch 379-----------------------------\n",
      "loss: 0.9217447509765625\n",
      "loss: 983.2115786895752\n",
      "loss: 1007.1654411373138\n",
      "loss: 956.9941045722961\n",
      "Avg loss: 1035.8302001953125!\n",
      "Epoch 380-----------------------------\n",
      "loss: 0.9277635498046874\n",
      "loss: 983.3606804962158\n",
      "loss: 1007.391942193985\n",
      "loss: 956.968036529541\n",
      "Avg loss: 1035.7027587890625!\n",
      "Epoch 381-----------------------------\n",
      "loss: 0.9240599975585938\n",
      "loss: 983.2686114654541\n",
      "loss: 1007.1470792160034\n",
      "loss: 956.6764337749481\n",
      "Avg loss: 1034.924072265625!\n",
      "Epoch 382-----------------------------\n",
      "loss: 0.9210767211914063\n",
      "loss: 982.6984004535675\n",
      "loss: 1007.0002104358673\n",
      "loss: 956.1050286884308\n",
      "Avg loss: 1034.9046630859375!\n",
      "Epoch 383-----------------------------\n",
      "loss: 0.9205859985351562\n",
      "loss: 982.5777541885376\n",
      "loss: 1006.7957344284058\n",
      "loss: 955.9635243453979\n",
      "Avg loss: 1034.7911376953125!\n",
      "Epoch 384-----------------------------\n",
      "loss: 0.917205322265625\n",
      "loss: 982.5871037101746\n",
      "loss: 1006.673891784668\n",
      "loss: 955.7858300170899\n",
      "Avg loss: 1034.5179443359375!\n",
      "Epoch 385-----------------------------\n",
      "loss: 0.9130672607421875\n",
      "loss: 982.4742848262787\n",
      "loss: 1006.5716142749786\n",
      "loss: 956.0259351825714\n",
      "Avg loss: 1034.7593994140625!\n",
      "Epoch 386-----------------------------\n",
      "loss: 0.9178123779296875\n",
      "loss: 982.5488181381226\n",
      "loss: 1006.7147189483643\n",
      "loss: 955.8438314857483\n",
      "Avg loss: 1034.69970703125!\n",
      "Epoch 387-----------------------------\n",
      "loss: 0.9168114013671875\n",
      "loss: 982.5594991779327\n",
      "loss: 1006.6231213436126\n",
      "loss: 955.769535812378\n",
      "Avg loss: 1034.404296875!\n",
      "Epoch 388-----------------------------\n",
      "loss: 0.9127371215820312\n",
      "loss: 982.5293623046875\n",
      "loss: 1006.6401747970581\n",
      "loss: 955.7944574546814\n",
      "Avg loss: 1034.4365234375!\n",
      "Epoch 389-----------------------------\n",
      "loss: 0.9124712524414063\n",
      "loss: 982.4960265216828\n",
      "loss: 1006.3982769851684\n",
      "loss: 955.512863286972\n",
      "Avg loss: 1033.9305419921875!\n",
      "Epoch 390-----------------------------\n",
      "loss: 0.9110841674804687\n",
      "loss: 982.1074165859222\n",
      "loss: 1006.2932437973022\n",
      "loss: 954.9707141151429\n",
      "Avg loss: 1033.6259765625!\n",
      "Epoch 391-----------------------------\n",
      "loss: 0.9066763916015625\n",
      "loss: 981.8953263530731\n",
      "loss: 1006.0919004096985\n",
      "loss: 955.0224066009522\n",
      "Avg loss: 1033.7933349609375!\n",
      "Epoch 392-----------------------------\n",
      "loss: 0.911566162109375\n",
      "loss: 982.096126630783\n",
      "loss: 1006.3949648971558\n",
      "loss: 955.0296114120483\n",
      "Avg loss: 1033.66064453125!\n",
      "Epoch 393-----------------------------\n",
      "loss: 0.9078038940429688\n",
      "loss: 981.9119572925567\n",
      "loss: 1006.0825317821502\n",
      "loss: 954.9001993713379\n",
      "Avg loss: 1033.5174560546875!\n",
      "Epoch 394-----------------------------\n",
      "loss: 0.9043797607421875\n",
      "loss: 981.9803429985046\n",
      "loss: 1005.9776905422211\n",
      "loss: 954.957016916275\n",
      "Avg loss: 1033.5548095703125!\n",
      "Epoch 395-----------------------------\n",
      "loss: 0.9034481811523437\n",
      "loss: 982.0747404346466\n",
      "loss: 1006.1260164527893\n",
      "loss: 954.7724484901428\n",
      "Avg loss: 1033.18408203125!\n",
      "Epoch 396-----------------------------\n",
      "loss: 0.9004533081054688\n",
      "loss: 981.7944657268524\n",
      "loss: 1005.781536863327\n",
      "loss: 954.9644844894409\n",
      "Avg loss: 1033.599853515625!\n",
      "Epoch 397-----------------------------\n",
      "loss: 0.904118896484375\n",
      "loss: 982.1633838691712\n",
      "loss: 1006.2231997737885\n",
      "loss: 954.8692838611603\n",
      "Avg loss: 1033.3057861328125!\n",
      "Epoch 398-----------------------------\n",
      "loss: 0.9015321655273437\n",
      "loss: 982.0539387302399\n",
      "loss: 1006.1226135425568\n",
      "loss: 954.7265703296662\n",
      "Avg loss: 1032.737548828125!\n",
      "Epoch 399-----------------------------\n",
      "loss: 0.8997999267578125\n",
      "loss: 981.5180864086151\n",
      "loss: 1005.8579413909912\n",
      "loss: 954.0356007881164\n",
      "Avg loss: 1032.5030517578125!\n",
      "Epoch 400-----------------------------\n",
      "loss: 0.8962312622070312\n",
      "loss: 981.294937379837\n",
      "loss: 1005.5085653896332\n",
      "loss: 954.2381391143799\n",
      "Avg loss: 1032.7916259765625!\n",
      "Epoch 401-----------------------------\n",
      "loss: 0.897191650390625\n",
      "loss: 981.6308056526184\n",
      "loss: 1005.81736195755\n",
      "loss: 954.1268160552978\n",
      "Avg loss: 1032.55322265625!\n",
      "Epoch 402-----------------------------\n",
      "loss: 0.8962695922851562\n",
      "loss: 981.5542840747834\n",
      "loss: 1005.86897000885\n",
      "loss: 954.0938934841156\n",
      "Avg loss: 1032.509765625!\n",
      "Epoch 403-----------------------------\n",
      "loss: 0.8936853637695312\n",
      "loss: 981.5924491710663\n",
      "loss: 1005.638011768341\n",
      "loss: 954.0709910736084\n",
      "Avg loss: 1032.111572265625!\n",
      "Epoch 404-----------------------------\n",
      "loss: 0.8885362548828125\n",
      "loss: 981.1738348445892\n",
      "loss: 1005.2856580085754\n",
      "loss: 954.1923118858338\n",
      "Avg loss: 1032.627685546875!\n",
      "Epoch 405-----------------------------\n",
      "loss: 0.8928689575195312\n",
      "loss: 981.9808475570678\n",
      "loss: 1006.1350962791442\n",
      "loss: 954.1677516155243\n",
      "Avg loss: 1032.1558837890625!\n",
      "Epoch 406-----------------------------\n",
      "loss: 0.8902213134765625\n",
      "loss: 981.2567209129334\n",
      "loss: 1005.358320072174\n",
      "loss: 953.9625231800079\n",
      "Avg loss: 1031.759033203125!\n",
      "Epoch 407-----------------------------\n",
      "loss: 0.8890399169921875\n",
      "loss: 981.0699527702332\n",
      "loss: 1005.2981484336854\n",
      "loss: 953.2312689151764\n",
      "Avg loss: 1031.4490966796875!\n",
      "Epoch 408-----------------------------\n",
      "loss: 0.8832359619140625\n",
      "loss: 980.9199969825745\n",
      "loss: 1005.2105822696686\n",
      "loss: 953.2458850746154\n",
      "Avg loss: 1031.3292236328125!\n",
      "Epoch 409-----------------------------\n",
      "loss: 0.8830191040039063\n",
      "loss: 980.9497521305084\n",
      "loss: 1005.0639656047821\n",
      "loss: 953.5656033821106\n",
      "Avg loss: 1031.68017578125!\n",
      "Epoch 410-----------------------------\n",
      "loss: 0.88653173828125\n",
      "loss: 981.1667610645294\n",
      "loss: 1005.3471790084839\n",
      "loss: 953.3196923789978\n",
      "Avg loss: 1031.2032470703125!\n",
      "Epoch 411-----------------------------\n",
      "loss: 0.8797632446289062\n",
      "loss: 980.9291229171753\n",
      "loss: 1005.0715261878968\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:8\u001B[0m\n",
      "Cell \u001B[1;32mIn [70], line 24\u001B[0m, in \u001B[0;36mcv_model\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-----------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 24\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     test_avg \u001B[38;5;241m=\u001B[39m test(test_X, test_y, model, loss_fn)\n\u001B[0;32m     26\u001B[0m     avg_losses\u001B[38;5;241m.\u001B[39mappend(test_avg)\n",
      "Cell \u001B[1;32mIn [69], line 11\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(X, y, model, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m      8\u001B[0m X_data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(X_data)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m      9\u001B[0m y_data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(y_data)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m---> 11\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(pred\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), y_data\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[0;32m     14\u001B[0m loss_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "Cell \u001B[1;32mIn [68], line 16\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 16\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear_relu_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m#x = torch.FloatTensor(x)\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_data = [4096]\n",
    "df_losses = pd.DataFrame()\n",
    "\n",
    "for n in n_data:\n",
    "    df = get_env_data(n)\n",
    "    df, dfY = get_dfs(df)\n",
    "\n",
    "    avg_losses = cv_model(df, dfY)\n",
    "\n",
    "    x = []\n",
    "\n",
    "    for i in avg_losses:\n",
    "        x.append(i.item())\n",
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "13"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "len(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.start = [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        self.state = self.start\n",
    "        self.score = 0\n",
    "        self.observation_space = spaces.Box(low=0, high=210, shape=(1,13))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state.append(action)\n",
    "        self.state = torch.tensor(self.state).cuda()\n",
    "        self.state = self.model.forward(self.state.float())\n",
    "        self.state = self.state.tolist()\n",
    "        reward = 0\n",
    "        reward += self.state[0]\n",
    "        if self.state[0] == 0:\n",
    "            self.state[0] += 0.1\n",
    "        reward -= 1000/self.state[0]\n",
    "        if 90 <= self.state[2] <= 100:\n",
    "            reward -= 10000\n",
    "        if self.state[1] - 0.5 > self.score:\n",
    "            self.score = self.state\n",
    "            reward += 10000\n",
    "\n",
    "        return self.state, reward, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        self.score = 0\n",
    "        return self.state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[138.16176 ,  34.262993, 157.0522  ,  86.56509 ,  61.958645,\n         78.408966,  39.83938 , 153.72751 , 208.01186 , 111.84863 ,\n         92.11368 , 209.64507 ,  23.819668]], dtype=float32)"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaces.Box(low=0, high=210, shape=(1, 13)).sample()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "Box([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], [[210. 210. 210. 210. 210. 210. 210. 210. 210. 210. 210. 210. 210.]], (1, 13), float32)"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"game_model/game_model_1\").cuda()\n",
    "\n",
    "env = CustomEnv(model)\n",
    "env.reset()\n",
    "\n",
    "env.observation_space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| time/                     |           |\n",
      "|    fps                    | 462       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 17        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -1.19e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00738   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 3         |\n",
      "|    policy_objective       | 0.0112    |\n",
      "|    value_loss             | 1.71e+10  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<sb3_contrib.trpo.trpo.TRPO at 0x18925b250a0>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sb3_contrib import TRPO\n",
    "\n",
    "model = torch.load(\"game_model/game_model_1\").cuda()\n",
    "\n",
    "env = CustomEnv(model)\n",
    "trpo = TRPO(\"MlpPolicy\", env, gamma=0.99, verbose=1)\n",
    "trpo.learn(total_timesteps=10_000, log_interval=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Int but found Float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [264], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(x)\n\u001B[0;32m      7\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mIntTensor)\n\u001B[1;32m----> 9\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m x\n",
      "Cell \u001B[1;32mIn [128], line 14\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 14\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear_relu_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\loool\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: expected scalar type Int but found Float"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"game_model/game_model_1\")\n",
    "x = torch.tensor(list(pd.read_csv(\"gamedata/start.csv\").drop([\"Unnamed: 0.1\", \"Unnamed: 0\", \"actions\"], axis=1).loc[0]))\n",
    "#model.forward(torch.tensor(x.tolist().append(2)))\n",
    "x = x.tolist()\n",
    "x.append(2)\n",
    "x = torch.tensor(x)\n",
    "x = x.type(torch.IntTensor)\n",
    "\n",
    "model.forward(x)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
