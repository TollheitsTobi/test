{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1><center>Projekt Seminar Deep Learning</center></h1>\n",
    "<h2><center>Deep Reinforcement Learning with ME-TRPO</center></h2>\n",
    "<center>Tobias Papen</center>\n",
    "<center><a href=\"https://github.com/TollheitsTobi/projektSeminar\">Simon Lausch, Jan Felix Fuchs & Paul Jansen haben sich mit Deep Q-Learning auseinander gesetzt.</a></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grober PLan\n",
    "1. Einleitung in das Projekt\n",
    "    - Was ist Reinforcement Learning (Agent, Environment, Reward function)\n",
    "    - Welchen Ansatz verwende ich und warum?\n",
    "    - Fragestellung für die Ergebnisse vom Projekt\n",
    "2. Environment\n",
    "    - Gym\n",
    "    - Freeway\n",
    "    - Observation\n",
    "    - Reward\n",
    "3. Model Ensemble\n",
    "    - Daten Sammeln\n",
    "    - Prepocessing\n",
    "    - Neuronales Netzwerk\n",
    "    - Training und Testen\n",
    "    - Custom Env\n",
    "4. ME-TRPO\n",
    "    - Wieso werden welche Parameter verwendet?\n",
    "5. Ergebnisse\n",
    "    - Tabelle aller Ergebnisse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Einleitung in das Projekt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Reinforcement Learning\n",
    "Reinforcement learning ist eine von drei machine learning Arten, neben supervised-und unsupervised learning. Alle reinforcement learning Ansätze haben die selben Grundbausteine und zwar: ein Environment, einen Agent, states, rewards und actions. Der Agent ist in einem state und muss eine action ausführen, das Environment gibt ihn dafür einen reward und einen neuen state zurück, so versucht der Agent den kumulativen reward zu maximieren. Man kann reinforcement learning noch weiter in model-free und model-based unterteilen, in model-free Ansätzen trainiert der Agent im richtigen Environment, während in model-based Ansätzen ein model des Environments erstellt und als simulation des echten Enironment benutzt wird. Man hat also den Vorteil das man wenn man zum Beispiel einen Roboter trainieren möchte die Zeit im realen Environment minimiert und so auch die chance für mögliche Hardware schäden minimiert.\n",
    "Irgendwas zur reward function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Welcher Ansatz?\n",
    "Wir haben uns in der Gruppe so aufgeteilt das Simon Lausch, Jan Felix Fuchs und Paul Jansen einen model-free Ansatz machen und ich einen model-based Ansatz. Normaler model-based Ansatz:\n",
    "1. Daten im echten Environment Sammeln\n",
    "2. Model mit den Daten trainieren\n",
    "3. Policy mit Backpropagation through time trainieren\n",
    "4. Wiederhole 3 bis es aufhört sich zu verbessern\n",
    "5. Wiederhole 1-4 bis es funktioniert\n",
    "\n",
    "Mein Ansatz ME-TRPO (Model Ensemble Trust Region Policy Optimization) hat zwei große unterschiede. Der Erste ist, dass man nicht ein model sondern mehrere models trainiert und als simulation des Environment verwendet. Der zweite unterschied ist, dass man nicht den Backpropagation through time Algorithmus zum trainieren sondern den Trust Region Policy Opimization Algorithmus verwendet.\n",
    "Der Vorteil von ME ist, dass es besser generalisiert als einzelne models und dadurch eine robustere Vorhersage liefern kann. TRPO optimiert die policy vom Agent, indem er sie in kleinen Schritten anpasst und sicherstellt, dass diese Schritte innerhalb einer festgelegten Trust Region bleiben. Das führt zu einer stabileren optimisierung und verhindert, dass sich die policy des Agents plötzlich stark verändert, was zu instabilen Verhaltensweisen führen könnte."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Projektziel\n",
    "In diesem Projekt untersuche ich was eine geeignete Anzahl an models ist und wie viele Schritte ME-TRPO weniger als ein model-free Ansatz benötigt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sb3_contrib import TRPO\n",
    "from gym import spaces\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Environment\n",
    "\n",
    "### 2.1 Gym\n",
    "Gym ist eine Open-Source-Library für Reinforcement Learning, die von OpenAI entwickelt wurde. Sie bietet viele verschiedene Environments an, die alle die selbe Struktur haben, so kann man sehr schnell viele verschiedene Environments benutzen. Eigene Environments kann man auch erstellen und so ist es möglich models für ME-TRPO einfach in ein Environment zu machen. Es gibt viele verschiedene libraries die auf gym aufbauen und die dokumentation ist auch sehr gut."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Freeway\n",
    "Als Environment in dem wir den Agent trainieren haben wir das Atari Spiel Freeway gewählt, in dem es darum geht das ein Huhn eine Straße mit insgesamt zehn Spuren so oft wie möglich in 2:30 Mintuen überquert. Je Spur fährt ein Auto mit unterschiedlichen Geschwindigkeiten aber immer in die selbe Richtung. Die einzigen actions die der Agent ausführen kann sind: nichts tun, hoch und runter. Freeway hat zwei Schwierigkeiten, bei der Schwierigkeit 0 wird der Agent wenn er angefahren wird ein paar pixel zurückgesetzt und bei der 1 wird er zum Start gesetzt. Es gibt verschiedene Modi die man spielen kann, wir verwenden Schwierigkeit 1 und Mode 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Observation\n",
    "Freeway hat drei verschiedene Arten von Daten die man aus dem Environment bekommen kann.\n",
    "1. RGB-Array:\n",
    "2. Grayscaled:\n",
    "3. RAM: 128 Bytes aus dem Spiel\n",
    "\n",
    "Ich benutze den RAM als Observation, aber ich verwende nicht alle 128 Byte, weil viele unnötige Daten dabei sind und ich einige auch nicht verstehe. Es werden die y Koordinate des Hühnchen, der Score, der Cooldown und die x Koordinaten der Autos benutzt. Die y Koordinate des Hühnchen liegt zwischen 6 und 176 wobei 6 der Start und 176 das Ziel ist. Der Cooldown wird nur in zwei Situationen gebraucht und zwar wenn das Huhn angefahren wird oder ins Ziel läuft. Die x Koordinaten der Autos liegen zwischen 0 und 159."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ObservationRAM(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Observation space muss am Anfang festgelegt werden\n",
    "        self.observation_space = spaces.Box(low=0, high=210, shape=(13, ))\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Die Daten die benutzt werden (y, score, cooldown, auto x pos)\n",
    "        obs = obs[[14, 103, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]].astype(float)\n",
    "\n",
    "        # normalisieren\n",
    "        obs[0] /= 176\n",
    "        obs[2] /= 142\n",
    "        for i in range(3, 13):\n",
    "            obs[i] /= 160\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Reward\n",
    "Eine gute Reward Function zu finden war schwer, weil der Agent entweder immer stehen geblieben oder nach oben gerannt ist. Wenn der Agent einfach nur nach oben rennt ohne auf Autos zu achten bekommt er ungefähr 12 Punkte. Das Ziel ist also eine Reward Function die den Agent fürs stehenbleiben und in Autos laufen bestraft. Bei meiner Reward Function bekommt der Agent wenn er ins Ziel läuft 10.000 Reward, wenn er in ein Auto läuft -1.000 und dann noch jeden step $\\frac{3}{2}y-90$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class LinearReward(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def reward(self, reward):\n",
    "        # RAM vom env um den reward zu berechnen\n",
    "        ram = self.env.unwrapped.ale.getRAM()\n",
    "        reward = 0\n",
    "        # Wenn man ins Ziel läuft, ist der Cooldown 140 oder 141\n",
    "        if 140 <= ram[106] <= 141:\n",
    "            reward += 10000\n",
    "        # reward für die y Position\n",
    "        reward += (ram[14] * 1.5) - 90\n",
    "        # Wenn man überfahren wird, ist der Cooldown zwischen 90 und 100\n",
    "        if 90 <= ram[106] <= 100:\n",
    "            reward -= 1000\n",
    "        return reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Model Ensemble"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Model des Environment\n",
    "Es gibt verschiedene Arten von models die man benutzten kann, zum Beispiel Gaussian process, Generalized method of moments oder Neuronales Netzwerk. Da dieses Projekt für das Projekt Seminar deep learning ist, werde ich Neuronale Netzwerke verwenden. Der Input ist der State und eine Action und der Output ist der neue State, also $f(S, A) = S'$. Das Neuronale Netzwerk hat 14 Input Neuronen, einen hidden layer mit 256 Neuronen und 13 Output Neuronen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class EnvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(14, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Daten sammeln\n",
    "Die Daten werden in dem normalen Freeway Environment in meinen Wrapper Klassen gesammelt. Der Agent soll die Daten selber sammeln, weil man dann genau die States trainiert in denen der Agent selber ist. In der Ersten Epoche hat der Agent noch nicht genug Daten um eigene Entscheidungen zu treffen, dafür habe ich eine Methode mit einem Bias Richtung nach oben gehen implementiert. Die gesammelten Daten werden in einem Dataframe gespeichert und der Dataframe wird zurückgegeben."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_action_sample():\n",
    "    # Random Actions wobei nach oben laufen am Wahrscheinlichsten ist\n",
    "    x = random.randint(0, 101)\n",
    "    if x < 90:\n",
    "        return 1\n",
    "    if x < 97:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def get_env_data(n):\n",
    "    # Warten bis das Model zum Daten sammeln gespeichert wurde\n",
    "    while len(os.listdir(\"game_model/trpo3/\")) == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    env_data = LinearReward(ObservationRAM(gym.make(\"ALE/Freeway-v5\", obs_type=\"ram\", render_mode=\"rgb_array\", difficulty=1, mode=3)))\n",
    "    model = TRPO.load(\"game_model/trpo3/tmp\", env=env_data)\n",
    "\n",
    "    observation = env_data.reset()\n",
    "    df = pd.DataFrame(observation).T\n",
    "    # Actions: 0: nichts, 1: up, 2: down\n",
    "    actions = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # model bestimmt action\n",
    "        action = model.predict(observation)\n",
    "\n",
    "        # in den ersten x läufen generiert das model keine actions und es wird auf die get_action_smaple methode zurückgegriffen\n",
    "        if isinstance(action, tuple):\n",
    "            action = get_action_sample()\n",
    "\n",
    "        # observation 0 wird vor der schleife gespeichert\n",
    "        if i != 0:\n",
    "            df.loc[len(df)] = observation\n",
    "        actions.append(action)\n",
    "        observation, reward, done, info = env_data.step(action)\n",
    "        if done:\n",
    "            observation = env_data.reset()\n",
    "    env_data.close()\n",
    "\n",
    "    # df wird mit den richtigen Daten fertiggestellt\n",
    "    df[\"actions\"] = actions\n",
    "    df = df.rename(columns={0: \"y\", 1: \"score\", 2: \"cooldown\",\n",
    "                            3: \"car1\", 4: \"car2\", 5: \"car3\", 6: \"car4\", 7: \"car5\", 8: \"car6\", 9: \"car7\", 10: \"car8\", 11: \"car9\", 12: \"car10\"})\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Daten verarbeiten\n",
    "Um die Neuronalen Netzte richtig trainieren zu können brauchen wir noch einen Dataframe mit den Ergebnissen. In dem Dataframe df ist der State und die Action und im dfY der nächste State."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def process_dfs(df):\n",
    "    df = df.tail(len(df) - 1)\n",
    "\n",
    "    dfY = df.copy()\n",
    "    dfY.drop([\"actions\"], axis=1, inplace=True)\n",
    "\n",
    "    dfY = dfY.drop(dfY.index[[0]])\n",
    "    df = df.drop(df.index[[len(df) - 1]])\n",
    "    dfY.index = df.index\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    dfY = dfY.reset_index(drop=True)\n",
    "    return df, dfY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Training und Testen\n",
    "Loss function: L1Loss\n",
    "Optimizer: Adam\n",
    "earlystopping: 10\n",
    "Ganzer Batch wird auf den GPU gespeichert"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(X, y, model, loss_fn, optimizer, batch_size):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    for i in range(round((len(y) / batch_size) + 0)):\n",
    "        # In jedem durchlauf wird der ganze batch auf dem gpu gespeichert\n",
    "        train_X = torch.from_numpy(X[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "        train_y = torch.from_numpy(y[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        # loss wird für jeden Datensatz berechnet\n",
    "        for k in range(min(batch_size, len(train_X))):\n",
    "            pred = model.forward(train_X[k].float())\n",
    "            loss += loss_fn(pred.to(torch.float32), train_y[k].to(torch.float32))\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # bp\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # return: avg loss\n",
    "    return loss_sum / len(y)\n",
    "\n",
    "\n",
    "def test(X, y, model, loss_fn, batch_size):\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(round((len(y) / batch_size) + 1)):\n",
    "            # In jedem durchlauf wird der ganze batch auf dem gpu gespeichert\n",
    "            test_X = torch.from_numpy(X[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "            test_y = torch.from_numpy(y[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "\n",
    "            # loss wird für jeden Datensatz berechnet\n",
    "            for k in range(min(batch_size, len(test_X))):\n",
    "                pred = model.forward(test_X[k].float())\n",
    "                loss = loss_fn(pred, test_y[k])\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "    loss_sum /= len(y)\n",
    "    print(f\"Avg loss: {loss_sum}!\")\n",
    "    return loss_sum\n",
    "\n",
    "def train_test_model(X, y, batch_size, learning_rate):\n",
    "    # X: input, y: target\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # Reset parameters\n",
    "    model = EnvNetwork().cuda()\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    last_test_avg = 10000\n",
    "    overfit = 0\n",
    "    epochs = 200\n",
    "\n",
    "    # Alle Dataframes zu numpy arrays\n",
    "    train_X = train_X.to_numpy()\n",
    "    test_X = test_X.to_numpy()\n",
    "    train_y = train_y.to_numpy()\n",
    "    test_y = test_y.to_numpy()\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train_l = train(train_X, train_y, model, loss_fn, optimizer, batch_size)\n",
    "        test_avg = test(test_X, test_y, model, loss_fn, batch_size)\n",
    "\n",
    "        # Wenn der Test avg schlechter als vorher ist, wird der last_test_avg nicht aktualisiert\n",
    "        # und der overfit counter erhöht. Falls der counter >= 10 wird das training frühzeitig abgebrochen\n",
    "        if test_avg > last_test_avg:\n",
    "            overfit += 1\n",
    "        else:\n",
    "            overfit = 0\n",
    "            last_test_avg = test_avg\n",
    "        if overfit >= 10:\n",
    "            break\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Custom Environment\n",
    "Ein Custom Environment von Gym braucht immer eine init, step und reset Methoden, in der init muss der Observation und Action Space initialisiert werden. In der step Methode werden die Neuronalen Netze bentutzt in dem wir ein random model aus der list nehmen und damit den nächsten State bekommen. In der step Methode wird wird der Reward berechnet und die reset Methode wird das Environment in den Anfangszustand zurückgesetzt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, models):\n",
    "        # n models\n",
    "        self.models = models\n",
    "        self.GAME_START = [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        # state ist am anfang der start punkt von jedem spiel\n",
    "        self.state = self.GAME_START.copy()\n",
    "        self.score = 0\n",
    "        # observation und action space müssen vorher festgelegt werden\n",
    "        # observation space: y, score, cooldown, auto x\n",
    "        self.observation_space = spaces.Box(low=0, high=210, shape=(13, ))\n",
    "        # action space: 1, 2, 3\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.step_counter = 0\n",
    "        self.done = False\n",
    "\n",
    "    def step(self, action):\n",
    "        # normalisieren\n",
    "        self.state[0] /= 176\n",
    "        self.state[2] /= 142\n",
    "        for i in range(3, 13):\n",
    "            self.state[i] /= 160\n",
    "        self.done = False\n",
    "        # state wird um action erweitert\n",
    "        self.state.append(action)\n",
    "        self.state = torch.tensor(self.state).cuda()\n",
    "        # eins der n models predictet den nächsten state\n",
    "        self.state = self.models[np.random.randint(0, len(self.models))].forward(self.state.float())\n",
    "        self.state = self.state.tolist()\n",
    "\n",
    "        # Daten wieder in die richtige form\n",
    "        self.state[0] *= 176\n",
    "        self.state[2] *= 142\n",
    "        for i in range(3, 13):\n",
    "            self.state[i] *= 160\n",
    "\n",
    "        # state runden\n",
    "        self.state = [round(i) for i in self.state]\n",
    "\n",
    "        # gleiche Reward function wie nin der LinearReward class\n",
    "        reward = (self.state[0] * 1.5) - 90\n",
    "        if self.state[0] == 0:\n",
    "            self.state[0] += 0.1\n",
    "        if 90 <= self.state[2] <= 100:\n",
    "            reward -= 10000\n",
    "        if self.state[1] - 0.5 > self.score or self.state[0] >= 176:\n",
    "            self.score = self.state[1]\n",
    "            reward += 10000\n",
    "        self.step_counter += 1\n",
    "        # Falls 2048 steps gemacht wurden, wird das spiel zurückgesetzt\n",
    "        if self.step_counter % 2048 == 0:\n",
    "            self.state = self.GAME_START.copy()\n",
    "            self.done = True\n",
    "        return self.state, reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.GAME_START.copy()\n",
    "        self.score = 0\n",
    "        return self.state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ME-TRPO\n",
    "Jede Epoche werden 4096 Samples gesammelt, dass sind genau zwei ganze Durchläufe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Avg loss: 0.06761843649569614!\n",
      "Avg loss: 0.04763362886326999!\n",
      "Avg loss: 0.03785652707219054!\n",
      "Avg loss: 0.016775329922315674!\n",
      "Avg loss: 0.014816545065762455!\n",
      "Avg loss: 0.013693206132799847!\n",
      "Avg loss: 0.012836588676433752!\n",
      "Avg loss: 0.012178255970040297!\n",
      "Avg loss: 0.011466561438368734!\n",
      "Avg loss: 0.010789033259537083!\n",
      "Avg loss: 0.010122357734302753!\n",
      "Avg loss: 0.009445200085523284!\n",
      "Avg loss: 0.008762717920426152!\n",
      "Avg loss: 0.008160233493245713!\n",
      "Avg loss: 0.007610488294190001!\n",
      "Avg loss: 0.007118725060347324!\n",
      "Avg loss: 0.0066481641912302936!\n",
      "Avg loss: 0.006198371833982735!\n",
      "Avg loss: 0.005878606788424399!\n",
      "Avg loss: 0.005541345140560189!\n",
      "Avg loss: 0.005197254449586912!\n",
      "Avg loss: 0.004942117388639474!\n",
      "Avg loss: 0.004715436192535758!\n",
      "Avg loss: 0.0045088013498095604!\n",
      "Avg loss: 0.004354714539799162!\n",
      "Avg loss: 0.004226104573203247!\n",
      "Avg loss: 0.004113636518801175!\n",
      "Avg loss: 0.004029250442162981!\n",
      "Avg loss: 0.003953606894847529!\n",
      "Avg loss: 0.003907254647321494!\n",
      "Avg loss: 0.0038654382589768084!\n",
      "Avg loss: 0.003839600553633158!\n",
      "Avg loss: 0.0038060814539412663!\n",
      "Avg loss: 0.0037670033755518592!\n",
      "Avg loss: 0.0037429056372824667!\n",
      "Avg loss: 0.003740261690892633!\n",
      "Avg loss: 0.003711380656459911!\n",
      "Avg loss: 0.0036889520937604353!\n",
      "Avg loss: 0.003668114361192463!\n",
      "Avg loss: 0.003648950590649427!\n",
      "Avg loss: 0.0036316193579577346!\n",
      "Avg loss: 0.003610259535775455!\n",
      "Avg loss: 0.0035955348441383383!\n",
      "Avg loss: 0.0035657865371060355!\n",
      "Avg loss: 0.0035543382630058825!\n",
      "Avg loss: 0.0035471148768670777!\n",
      "Avg loss: 0.003535729148378448!\n",
      "Avg loss: 0.003526822963111387!\n",
      "Avg loss: 0.003515208065588379!\n",
      "Avg loss: 0.003509420824388239!\n",
      "Avg loss: 0.0034949845588245023!\n",
      "Avg loss: 0.003499930900954308!\n",
      "Avg loss: 0.0034862177053628777!\n",
      "Avg loss: 0.0034743857129800864!\n",
      "Avg loss: 0.0034690579316139737!\n",
      "Avg loss: 0.0034687936466608203!\n",
      "Avg loss: 0.003468137047502766!\n",
      "Avg loss: 0.0034460201675437325!\n",
      "Avg loss: 0.003464944275996275!\n",
      "Avg loss: 0.003461347255892777!\n",
      "Avg loss: 0.003464318106135668!\n",
      "Avg loss: 0.0034547818795145104!\n",
      "Avg loss: 0.0034586258283954945!\n",
      "Avg loss: 0.003458077378906659!\n",
      "Avg loss: 0.003433642970632654!\n",
      "Avg loss: 0.003453002125796128!\n",
      "Avg loss: 0.0034604409665382123!\n",
      "Avg loss: 0.0034403363067566136!\n",
      "Avg loss: 0.003431898640315517!\n",
      "Avg loss: 0.0034399032455380723!\n",
      "Avg loss: 0.0034265602458092344!\n",
      "Avg loss: 0.0034344781332004788!\n",
      "Avg loss: 0.0034178127088516958!\n",
      "Avg loss: 0.0034115818161564366!\n",
      "Avg loss: 0.0034138985534519955!\n",
      "Avg loss: 0.003394411820516842!\n",
      "Avg loss: 0.0034010178558607045!\n",
      "Avg loss: 0.0033753297187556677!\n",
      "Avg loss: 0.00338341748578925!\n",
      "Avg loss: 0.00339132530080207!\n",
      "Avg loss: 0.0033736097283632504!\n",
      "Avg loss: 0.0033724349591965127!\n",
      "Avg loss: 0.003365981671264239!\n",
      "Avg loss: 0.0033556105103813376!\n",
      "Avg loss: 0.003338272453248705!\n",
      "Avg loss: 0.003332301935862092!\n",
      "Avg loss: 0.003329559806699134!\n",
      "Avg loss: 0.0033315989264281727!\n",
      "Avg loss: 0.0033325820435601283!\n",
      "Avg loss: 0.003319791201563747!\n",
      "Avg loss: 0.0033174115553272843!\n",
      "Avg loss: 0.0032921270977524925!\n",
      "Avg loss: 0.003291155143597574!\n",
      "Avg loss: 0.0032922206597728134!\n",
      "Avg loss: 0.003277503754568934!\n",
      "Avg loss: 0.003278772873500407!\n",
      "Avg loss: 0.003276749331569743!\n",
      "Avg loss: 0.003276513321751525!\n",
      "Avg loss: 0.0032768674257173842!\n",
      "Avg loss: 0.0032795657492608664!\n",
      "Avg loss: 0.0032675217381385236!\n",
      "Avg loss: 0.0032541391563777676!\n",
      "Avg loss: 0.0032634866829548!\n",
      "Avg loss: 0.0032691950473441016!\n",
      "Avg loss: 0.00324644554160357!\n",
      "Avg loss: 0.0032538640545434973!\n",
      "Avg loss: 0.003252485472880196!\n",
      "Avg loss: 0.003266237186038261!\n",
      "Avg loss: 0.0032623789027909516!\n",
      "Avg loss: 0.0032712462770836965!\n",
      "Avg loss: 0.003266002786994512!\n",
      "Avg loss: 0.0032729475689372498!\n",
      "Avg loss: 0.0032430688276874897!\n",
      "Avg loss: 0.0032430688325160253!\n",
      "Avg loss: 0.003248072014671716!\n",
      "Avg loss: 0.003260582453799751!\n",
      "Avg loss: 0.0032477106594206774!\n",
      "Avg loss: 0.0032364504434183603!\n",
      "Avg loss: 0.003238453149541296!\n",
      "Avg loss: 0.003271756631843958!\n",
      "Avg loss: 0.0032602190345373603!\n",
      "Avg loss: 0.0032326868828590153!\n",
      "Avg loss: 0.003237677310663176!\n",
      "Avg loss: 0.0032363841479224336!\n",
      "Avg loss: 0.0032394005970639065!\n",
      "Avg loss: 0.00325434997171515!\n",
      "Avg loss: 0.0032717245291155545!\n",
      "Avg loss: 0.0032429957274148023!\n",
      "Avg loss: 0.003231601854065016!\n",
      "Avg loss: 0.003233967828590886!\n",
      "Avg loss: 0.003237194831732278!\n",
      "Avg loss: 0.0032216948563440016!\n",
      "Avg loss: 0.0032152189993134776!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "N_DATA = 8192\n",
    "N_MODELS = 5\n",
    "ITERATIONS_PER_EPOCH = 200\n",
    "STEPS_PER_ITERATION = 2048\n",
    "EPOCHS = 25\n",
    "trpo = TRPO(\"MlpPolicy\", LinearReward(ObservationRAM(gym.make(\"ALE/Freeway-v5\", obs_type=\"ram\", render_mode=\"rgb_array\", difficulty=1, mode=3))), gamma=0.99, verbose=1)\n",
    "#trpo = TRPO.load(\"game_model/models/model_4\", env=LinearReward(ObservationRAM(gym.make(\"ALE/Freeway-v5\", obs_type=\"ram\", render_mode=\"rgb_array\", difficulty=1, mode=3))))\n",
    "trpo.save(\"game_model/trpo3/tmp\")\n",
    "\n",
    "data = []\n",
    "#data.append(pd.read_csv(\"game_model/data/data.csv\").drop([\"Unnamed: 0\"], axis=1))\n",
    "scores = []\n",
    "\n",
    "def make_models(n):\n",
    "    df = get_env_data(N_DATA)\n",
    "\n",
    "    scores.append(max(df[\"score\"]))\n",
    "\n",
    "    data.append(df)\n",
    "    df = pd.concat(data)\n",
    "    df.to_csv(\"game_model/data/data3.csv\")\n",
    "\n",
    "    df, dfY = process_dfs(df)\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for i in range(n):\n",
    "        models.append(train_test_model(df, dfY, learning_rate=0.0014335761400056032, batch_size=234))\n",
    "    return models\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    models = make_models(N_MODELS)\n",
    "    env = CustomEnv(models)\n",
    "\n",
    "    while len(os.listdir(\"game_model/trpo3/\")) == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "    trpo = TRPO.load(\"game_model/trpo3/tmp\", env=env)\n",
    "    os.remove(\"game_model/trpo3/tmp.zip\")\n",
    "    while len(os.listdir(\"game_model/trpo3/\")) == 1:\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"Epoche: {i}\\n________________________________________________________\")\n",
    "    trpo.learn(total_timesteps=ITERATIONS_PER_EPOCH*STEPS_PER_ITERATION, log_interval=25)\n",
    "    trpo.save(f\"game_model/trpo3/tmp\")\n",
    "    trpo.save(f\"game_model/models/model3_{i}\")\n",
    "\n",
    "trpo.save(f\"trpo_models/model_3\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
